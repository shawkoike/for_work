<p><a href="https://zo7.github.io/blog/2016/09/25/generating-faces.html" rel="nofollow" title="" class="ext-link">Generating Faces with Deconvolution Networks</a> （2016-9-25） by <a href="https://zo7.github.io/about/" rel="nofollow" title="" class="ext-link">Flynn</a></p><p>私の好きなディープラーニングの論文の1つが<a href="https://arxiv.org/abs/1411.5928v3" rel="nofollow" title="" class="ext-link">ディープラーニングにおける畳み込みネットワークを使ったイス、テーブル、車の生成</a>です。論旨はとても単純で、ネットワークに自分が描きたい物のパラメータを与えると、そのとおりにできあがるというものです。しかし、そこからは信じられないほど面白い結果が生じます。ネットワークに3D空間や自らが描く物体の構造についての概念を学習する能力があるようで、数値ではなく画像が生成されるので、ネットワークの”考え”方までもがよく分かる感じがします。</p><p><iframe width="560" height="315" src="https://www.youtube.com/embed/QCSW4isBDL0" frameborder="0" allowfullscreen></iframe></p><p><em>注釈: イスのモーフィング</em></p><p>少し前に<a href="http://www.socsci.ru.nl:8180/RaFD2/RaFD" rel="nofollow" title="" class="ext-link">Radboud表情データベース</a>をたまたま見つけ、これと同じことが表情の生成や補間に適用できるのではないかと思いました。</p><p>結果を見て本当に興奮しました！</p><p><img src="https://zo7.github.io/img/2016-09-25-generating-faces/animation.gif" alt="" width="507px"></p><p>この実装にあたって、先ほどのイスの論文から”1s-S-deep”モデルのバージョンを応用しました。このモデルにおいては、イスの”スタイル”のワンホットエンコーディングと、方向とカメラの位置を決めるためのパラメータがネットワークへ入力されます。そのあと、入力されたデータをいくつかの全結合層に通し、描くものの表現を得ます。その表現が逆畳み込みネットワークへ渡され、画像を描き、セグメンテーションマスクを予測します。表情の生成のため、セグメンテーションネットワークを完全に取り除く以外（理由は顔のグランドトゥルースがないためです）、論文と同じことを行い、人物や感情、顔の向きを入力しました。</p><p><img src="https://zo7.github.io/img/2016-09-25-generating-faces/chairs-model.png" alt="A diagram of the model used to generate chairs (from
        Dosovitskiy et al.)"><br>
<em>イスの生成に使われたモデルのダイアグラム（Dosovitskiy et al.）</em></p><p>ここでの逆畳み込みネットワークは、セマンティックなセグメンテーションと生成ネットワーク（例えば、<a href="https://arxiv.org/abs/1406.2661" rel="nofollow" title="" class="ext-link">こちら</a>、<a href="https://www.cs.nyu.edu/~gwtaylor/publications/zeilertaylorfergus_iccv2011.pdf" rel="nofollow" title="" class="ext-link">こちら</a>、または<a href="http://arxiv.org/abs/1505.04366" rel="nofollow" title="" class="ext-link">こちら</a>）向けの他のモデルで人物や感情を見る方法と似ています。本質的に、分類ネットワークで一般的に使用される操作の逆です。通常は、畳み込みが数層あり、続いて入力量の次元を減らすためのプーリング層があります。逆畳み込みネットワークでは、これらの操作を逆に行います。まず入力のアップサンプリング（別名：アンプーリング）を行い、<em>それから</em>畳み込みを適用するのです。</p><p><img src="https://zo7.github.io/img/2016-09-25-generating-faces/deconv.png" alt="An illustration of 'deconvolution' as upsampling (unpooling)
        followed by a convolution operation (from Dosovitskiy et al.)"><br>
<em>アップサンプリング（アンプーリング）の後に畳み込み操作が続く’逆畳み込み’の図（Dosovitskiy et al）</em></p><p>本質的に、アンプーリングを行う際はキャンバス上でグリッド状に点をペイントし、それから畳み込みカーネルをペイントブラシとして使い、ペイントした色を周りに広げて混ぜます。</p><p>このアーキテクチャを論文からほぼそのまま適用できましたが、GPUメモリを消耗せずにより高解像度のイメージを取得するために、層ごとのカーネルの数を調整し、バッチ正規化を使用して確実にLeaky ReLU活性化関数が動作するようにしなければなりませんでした。モデルは、TheanoとTensorFlow上に構築された高度なディープラーニングフレームワークの<a href="https://keras.io/" rel="nofollow" title="" class="ext-link">Keras</a>に実装されます。</p><p>既にご覧いただいたとおり、ネットワークはかなりスムーズに人物と人物の間や感情と感情の間を補間することができます。</p><p><iframe width="560" height="315" src="https://www.youtube.com/embed/UdTq_Q-WgTs" frameborder="0" allowfullscreen></iframe></p><p><em>注釈：感情と人物の循環</em></p><p>実際にモード間をやや写実的に補間し、各サンプル間のフェージングのような不自然なメソッドに頼ることはありません。私が本当に驚いたのは、ある意味で口の開閉のような顔の特徴について学んでいるみたいだったということです。頬骨の動きや眉毛の上下の動きなどを見ることができます。もしかすると、これを使って複雑な式と遷移をアニメーション化することができるかもしれません。</p><p>人物のみ、または感情のみに分けて、補間する様子を見ることもできます。</p><p><iframe width="560" height="315" src="https://www.youtube.com/embed/VpFbpMPLKSQ" frameborder="0" allowfullscreen></iframe></p><p><em>注釈: 人物の補間</em></p><p><iframe width="560" height="315" src="https://www.youtube.com/embed/QzYzf9kuCYQ" frameborder="0" allowfullscreen></iframe></p><p><em>注釈: 感情の補間</em></p><p>顔の向きの変化を補間させることについては断念しました。残念ですが、ネットワークは同じ方法で顔の向きを学習することができなかったのです。その理由として可能性が高いと思われるのは、データセット内にあった利用可能な向きの情報はネットワークが3D空間の感覚を発展させるには粗すぎたことと（データセットには45度毎の向きしか含まれません）、より簡単な方法を選んで様々な向きの顔を描く学習していたことです。</p><p><iframe width="560" height="315" src="https://www.youtube.com/embed/F4OFkN3EURk" frameborder="0" allowfullscreen></iframe></p><p><em>注釈: 向きの補間</em></p><p>特に、後頭部などの全く知識のないものを描こうとした際のネットワークの振る舞いは、視覚的に非常に興味深いものです。ネットワークを利用した際に、新しいながら根本的に不正確で、それでも視覚的に面白いイメージを創り出す方法はおそらく他にもあるでしょう。</p><p>今のところ、ネットワークに渡されたパラメータの入力値はすべて、おおむね”有効”な値でした。これまで、人物と感情のベクトルは常に正規化されていました（つまり、人物と感情を混合した場合でもです）。そして、向きの入力値は常に有効な角度を表していました。しかし、このルールを破り、代わりにランダムな値をネットワークに渡すと、どうなるでしょうか？</p><p>とても恐ろしいことになります。</p><p><img src="https://zo7.github.io/img/2016-09-25-generating-faces/random-1.jpg" alt=""><br>
<img src="https://zo7.github.io/img/2016-09-25-generating-faces/random-2.jpg" alt=""></p><p>無効な向きの解釈の方法が分からなかったり、処理する人物と感情が”多すぎ”たりといった、いくつかの要素が合わさることで、ネットワークは無秩序で異様な状態に、顔を伸ばしたり、ねじ曲げたりしてしまうのです。</p><p>フレームごとに入力値を少しずつランダムに変化させることで、アニメーションも作ることができます。</p><p><iframe width="560" height="315" src="https://www.youtube.com/embed/vt8zNvJNjSo" frameborder="0" allowfullscreen></iframe></p><p><em>注釈: ランダムパラメータ</em></p><p>ワイルドですね。</p><p>最後にご紹介するのは、もっと面白いイメージを作るために、部分的に訓練したネットワークを利用して色々試す方法です。例えば、部分的に訓練したネットワークでAdaGradを使うと、以下のようにランダムなイメージが作られます。</p><p><img src="https://zo7.github.io/img/2016-09-25-generating-faces/adagrad.jpg" alt=""></p><p><iframe width="560" height="315" src="https://www.youtube.com/embed/d1g5ALhjoAk" frameborder="0" allowfullscreen></iframe></p><p><em>注釈: 部分的に訓練したネットワークでAdaGradを使用</em></p><p>私が特に関心を持ったのは、異なるオプティマイザが、どれだけ違うイメージを描くのかという点です。特に、訓練初期は興味深いです。例えば、部分的に訓練したネットワークで、単純な確率的勾配降下法を使って生成されるイメージは、人の顔というよりも抽象画のように見えます。</p><p><img src="https://zo7.github.io/img/2016-09-25-generating-faces/sgd.jpg" alt=""></p><p><iframe width="560" height="315" src="https://www.youtube.com/embed/wMpsOO_qpjw" frameborder="0" allowfullscreen></iframe></p><p><em>注釈: 部分的に訓練したネットワークで確率的勾配降下法を使用</em></p><p>私は、これらのネットワークを使った実験や、イメージ生成やオプティマイザの影響をネットワークが学習する過程の観察を、もっと詳しく行いたいと思っています（そして、どうしてその学習過程を取るのかという理由も知りたいですね）。しかし、今後の投稿のために今回は触れないでおきましょう。</p><p><a href="https://github.com/zo7/deconvfaces" rel="nofollow" title="" class="ext-link">コードはこちらでご覧いただけます</a>。</p><p><em>2016年10月2日改：モデルが実際に行っていることを正確に反映させるため、畳み込み演算の記述を変更しました。”スイッチ”のアンプールを学習するのではなく、アンプーリングするためにアップサンプリングを行っています。</em></p><p><em>2016年9月30日改：部分的に訓練したネットワークのアニメーションを追加しました。</em></p>
