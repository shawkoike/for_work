Memory use and speed of JSON parsers（2015-11-25）byIonel Cristian Mărieș私は、多数の大容量のデータをあちこちに移動させなければならない(クライアント端末をHTTP APIに接続してデータを取得します)ような特殊な使用事例を扱っています。なぜだか1、転送形式にはJSONが使われていました。ある時、その大容量のデータが、さらに巨大になったのです。数百メガバイトどころではありません。JSONのデコード処理を実行すると大量のRAMが使用されることが分かりました。たった240MBのJSONペイロードで4.4GBですよ。信じられません。2組み込みのJSONライブラリを使っていて、まず「もっと性能の良いJSONパーサがあるはずだ」と思いました。そんなわけで、計測を始めたのです。さて、メモリ使用量の計測はやっかいです。psコマンドを使ったり、/proc/&lt;pid&gt;を見たりすることはできますが、断片的なスナップショットが得られるだけで、実際の最大使用量を求めることは難しいでしょう。幸いなことに、Valgrindは、どんなプログラムでもメモリの割り当てを追跡することができますし(カスタムメモリアロケータを使うためにすべて再コンパイルするのとは対照的に)、massifという素晴らしいツールもあります。そこでValgrindを使ってちょっとしたベンチマークの作成に取り掛かりました。入力はこんな感じです。私のアプリケーションで問題となっているデータと非常によく似た構造を持つ240MBのJSONを得ることができました。valgrind --tool=massif --pages-as-heap=yes --heap=yes --threshold=0 --peak-inaccuracy=0 --max-snapshots=1000 ...を実行します。Python 2.7上では各パーサについて次のような結果が得られます。(Python 3.5上の結果については下にスクロールしてください)。結果をご覧ください。私のサンプルデータがおかしいのではとおっしゃるかもしれません。しかし残念ながら、このようなデータに遭遇する場合があるのです。時折、わずかな文字列が恐ろしいほどの大きさの容量に拡大するのです。jsonは重大な脆弱さをはらんでおり、入力の十数倍のメモリを必要とします。こんな結果になるとは。cjsonを使うようにという結果を突き付けられました。VeryBadBugs™ を含んでいるうわさが出ていますが3、バグトラッカーの不足が、このプロジェクトを全く味気のないものにしているのだと思います。rapidjson は、新く参入してきたパーサです4。しかし、Python 2 バインディングは、肝心な  部分が  欠けて  いるようです。それでも、これがどのように動くのか、少なくともその考え方を知るのは興味深いことです。Python 3-onlyバインディングの方が完成度が高そうに見えます。しかし、残念ながら、今のところこのアプリケーションはPython 2上でしか動作しません。yajlとujsonは、十分に完成度が高いにもかかわらず多くのメモリを食います。もっと良い方法があるはずです…。何を選んでも短所があるようです。ここにぴったりの格言があります5 。顧客が「何かを必要としている」と頼むとき、本当に必要としているものは、要求しているものよりももっとシンプルで低コストのものなのです。要件についてよく話し合い、精査すれば、問題の多くはその時点で解決します。これはそのような状況なのです。私のケースでJSONが全く必要なかったともっと早く気付いていれば…。HTTP APIのフォーマットを変えるにはまだまだ手直しが必要です。しかし、cjsonやrapidjsonのバインディングを自力でメンテナンスしたり修正したりするよりはマシです。msgpackを試してみたところ（さらに、怖いもの見たさで他の古いものも6）、このような結果が出ます。テストプログラムを見ると、msgpackで非常に特殊なオプションが使われていることに気付くでしょう。その理由は、Msgpackの初期バージョンが文字列の扱いをあまり得意としていなかったからで(扱う文字列型が1つでした 7)、特殊なオプションが必要なのです。Python 2では、Python 3では、
  * bytes は、バイナリになります。
  * strは、文字列になります。pytest-benchmarkを使った結果8です。最短の処理時間だけを表示しました。テストの目的に合わせて実行した結果ですが、他に気になることがあれば、テストプログラムをご自分のコンピュータで試してください。問題を抱えた私のアプリケーションをPython 2の上だけで走らせるのは、まともな（と同時に悲しい）理由があってのことです。しかし、最新で最高の環境下でどうなるのかを探って自ら墓穴を掘る理由はありません。そのうち誰かが移植するでしょう…。Python 3にはcjsonやjsonlibがありません。jsonlib2が生まれた背景すらわかりません。Msgpackを使う方が無難なようです。この実験は、非常に偏ったデータを使っています。完全に例外的なデータ形式だと言う人がいるかもしれません。ですので、テストプログラムを使って、ご自身のデータでベンチマークすることをお勧めします。しかし、ベンチマークが面倒なら、異なる種類のデータを使った結果がいくつかあります。これは、単に、入力データでメモリ使用量と処理時間がどのくらい変わるかを知るために行なった結果です。189MBのcitylots.jsonでは驚くほど違う結果が出ます。小容量のオブジェクトでは、明らかにsimplejsonが他よりも優れており、Python3上ではjsonの結果が大幅によくなっています。処理時間について。2.2MBというとても小さなcanada.jsonでは、さらに異なる結果が出ます。メモリ使用量は重要な指標とは言えません。処理時間は、またしても異なる結果が出ます。こういう結果だからfreelistsといううまい利用法が考えられたのかも？処理速度もメモリ使用量もデータの構造に左右されます。処理速度がメモリ使用量に必ずしも比例するというわけではありません。繰り返しますが、上記の数値を鵜呑みにしないで、ご自分のデータを使って自らベンチマークを行って下さい。たとえ、あなたのデータの形式が私のものと全く同じものであっても、あなたのコンピュータは私のコンピュータとは違った動きをするでしょう。あなたのコンピュータ（例えば、アーキテクチャ、共有ライブラリが異なります）上ではメモリ使用量すら違います。その上で、ベンチマークで使用した私のデータのどれかとあなたのデータがきっちり同じ形式になる見込みはあるでしょうか？テスト環境のセットアップ上記セットアップは完璧ではありません。もし本当に関心があるのでしたら、テストプログラムをお使いください。メモ
要約：ペイロードが大きいデコード中心の利用において、JSONデコーダーは、たびたび過度のメモリを使います。私はJSONをあきらめてMsgpackに変えました。ご自分でテストプログラムを走らせてみて、ご自身の中での結論は決めてください。⸻いただいたいろいろなフィードバックを基に9 Valgrindの代わりにru_maxrssを使い、さらにいくつか実装してベンチマークを行いました。最新の結果はこちらです。
