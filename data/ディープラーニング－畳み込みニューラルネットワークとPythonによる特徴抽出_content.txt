Deep learning – Convolutional neural networks and feature extraction with Python （2015-08-19） by Christian S. Perone畳み込みニューラルネットワーク（またはConvNet）は、生物学から着想を得た多層パーセプトロン（MLP）の変形です。畳み込みニューラルネットワークには種類の異なる様々な層があり、各層は通常のMLPとは異なる働きをします。ConvNetについて詳しく学びたい方には、CS231n 視覚認識のための畳み込みニューラルネットワークのコースをお勧めします。以下の図は、畳み込みニューラルネットワークのアーキテクチャを表しています。
訳：入力層→隠れ層1→隠れ層2→出力層標準的なニューラルネットワーク（CS231nのWebサイトより）畳み込みニューラルネットワークのアーキテクチャ（CS231nのWebサイトより）この図で分かるように、ConvNetは3次元の量および3次元の量への変換によって機能します。CS231nのチュートリアル全てをここで繰り返すつもりはありませんが、もし興味があれば、この続きを読む前にチュートリアルを一読してください。私が気に入って使っているディープラーニング用のPythonのパッケージの1つがLasagneとnolearnです。LasagneはTheanoをベースにしているので、GPUのスピードアップにより非常に大きな違いが出ます。さらに、ニュートラルネットワークの構築に宣言的アプローチを使えるのでとても便利です。nolearnライブラリは、ニューラルネットワークのパッケージ（Lasagneを含む）周辺のユーティリティを集めたものです。これは、ニューラルネットワークのアーキテクチャを構築する際、層の検査などを行う時にとても役に立ちます。この投稿では、畳み込み層とプーリング層を用いたシンプルなConvNetアーキテクチャの構築方法をご紹介したいと思います。また、ConvNetを使ってどのように特徴抽出機構をトレーニングし、サポートベクターマシン（SVM）やロジスティック回帰といった異なるモデルに与える前の特徴をどのように抽出するかという方法もご紹介します。多くの場合、事前トレーニングされたConvNetモデルを使い、特徴を抽出するために、ImageNetのデータセットでトレーニングされたConvNetから最終的な出力層を取り除きます。これは普通、転移学習と呼ばれています。というのも、異なる問題用の特徴抽出機構として、他のConvNetの層を使うことができるからです。ConvNetの最初の層のフィルタはエッジ検出器として機能するので、他の問題用の一般的な特徴検出機構として使うことができます。MNISTデータセットとは手書きの数字を分類する最も古典的なデータセットの1つです。ここではPython用にpickle化したバージョンを使いますが、まず、必要なパッケージをインポートしましょう。ご覧のとおり、描画のためのmatplotlib、MNISTデータセットをダウンロードするPythonのネイティブモジュール、numpy、theano、lasagne、nolearnと、モデル評価用のscikit-lean関数がインポートされます。その後、MNISTの読み込み関数を定義します（これはLasagneチュートリアルで使ったものとほぼ同じ関数です）。ここではMNISTのpickle化されたデータセットをダウンロードし、それを3つの異なるデータセット（train、validationとtest）に分割しています。その後、画像コンテンツをreshapeし、後でLasagne 入力層に入力する準備をします。GPU/theanoにはデータ型に制限があるので、数値配列型をuint8に変換します。MNISTデータセットを読み込み、inspectする準備ができました。上記のコードにより下記のイメージが出力されます（IPython Notebookを使っています）。MNISTの数の例（この場合は5）さあConvNetアーキテクチャを定義し、GPU/CPUを使ってトレーニングしましょう（私のGPUは安物ですが、とても役立っています）。パラメータ層の中では、層の名称/型でタプルの辞書を定義し、そして、この層のパラメータを定義します。ここでのアーキテクチャは2つの畳み込み層とプーリング、全結合層（密層）、そして出力層を使っています。層の間にはドロップアウトもあります。ドロップアウト層はランダムに入力値を0に設定して過剰適合を防ぐための正則化項です（下記の画像を参照してください）。
ドロップアウト層の効果（CS231nのWebサイトより）トレーニングメソッドを呼び出した後、nolearnパッケージが学習プロセスのステータスを示します。質素なGPUの搭載された私のマシンでは、下記のような結果になりました。最終的な正確性は0.98526でした。10エポックのトレーニングにしてはかなりいいパフォーマンスです。これでデータセットの全テスト結果を予測するために、このモデルを使用することが可能となりました。そこから、またニュートラルネットワーク分類のパフォーマンスをチェックするために、混同行列をプロットすることができます。上記のコードは、次のような混同行列をプロットします。
混同行列ご覧のように、対角項に分類がより密集しており、分類器のパフォーマンスが上々であることを表しています。次は最初の畳み込み層から32のフィルタを視覚化します。上記のコードは、次のようなフィルタを描画します。
最初の層、5x5x32フィルタご覧のように、nolearnの plot_conv_weightsが指定した層に存在する全てのフィルタを生成します。今度はいよいよtheanoでコンパイルした関数を作成し、あなたが意図する層の所までアーキテクチャに入力値をフィードフォワードします。出力層を求めるための関数と出力層の前にある密層を求めるための関数を取得します。ご覧のように、（出力層と密層を求めるための）f_outputとf_denseという2つのtheano関数を得ました。注意してほしいのは、ここで層を得るために、“deterministic”という追加パラメータを使用しているということです。これはフィードフォワードパスに影響するドロップアウト層を避けるためです。ここでサンプルのinstanceを入力フォーマットに変え、出力層を求めるためのtheano関数にフィードします。ご覧のように、f_output関数は平均858マイクロ秒かかりました。instanceの出力層の活性化を描画することができます。上記のコードは次のようにプロットされます。
出力層の活性化ご覧のように、この手書き数字は、7と認識されました。「ネットワークのどの層を求めるにもtheano関数を作成できる」ということが、とても有効であることが分かります。というのも（以前やったように）、出力層の1つ前の密層の活性化を求めるために関数を作ることができ、その活性化を特徴として活用でき、分類器ではなく、特徴抽出機構としてニューラルネットワークを使用することができるからです。続いて、密層のための256ユニットの活性化をプロットしてみましょう。上記のコードは、以下のようなプロットを作成します。
密層の活性化この256の活性化の出力値を、ロジスティック回帰やSVMといった線形分類器で入力する特徴として使用できます。チュートリアルを楽しんでいただけましたでしょうか？
