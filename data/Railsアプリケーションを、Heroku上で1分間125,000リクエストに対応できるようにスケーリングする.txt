<p><a href="https://zeemee.engineering/scaling-rails-to-125-000-requests-per-minute-on-heroku-b4128a10a769#.fizsbwtek" rel="nofollow" title="" class="ext-link">Scaling Rails to 125,000 Requests per Minute on Heroku</a> （2016-5-3） by <a href="https://zeemee.engineering/@phuitsing" rel="nofollow" title="" class="ext-link">Pete Huitsing</a></p><p><em>要約： Herokuでは、Railsのスケーリングが非常に簡単できますが、考慮すべき重要なポイントがあります。DynoとPostgresのさまざまな設定がHerokuでのパフォーマンスにどのように影響を与えるかについて調べました。</em></p><p><a href="https://www.zeemee.com/" rel="nofollow" title="" class="ext-link">ZeeMee</a>のサーバは、今秋、来たる大学入学のシーズンにリクエストが殺到する見込みです。学生はZeeMeeを使って入学願書に動画や写真を付加することができるので、当社のWebサービスが受ける負荷は非常に深刻なものになっています。ピーク時（大学入学の出願期限）には、リクエストのロードが平均の150倍にもなります。</p><p>今年の秋を迎えるにあたり、APIのリクエストの爆発的増加にしっかり備えておこうと考え、Herokuでパフォーマンスがどこまで上げられるかを検討することにしました。当社のアプリはPOJA（plain old JSON API）とGraphQLのエンドポイントを提供しています。私たちのWebアプリ・モバイルアプリでは、これらのAPIで読み込みと書き込みのクエリを行います。</p><p>第一段階として、”平均的なユーザ”がZeeMeeを使う際に何をしているかを調べました。ざっと挙げるとZeeMeeのユーザは以下のようなことをしています。</p><p>このような平均的なユーザプロファイルを、<a href="https://github.com/flood-io/ruby-jmeter" rel="nofollow" title="" class="ext-link">jmeter-ruby</a>を使ってJMeterスクリプトにしました。そのあと<a href="https://flood.io/" rel="nofollow" title="" class="ext-link">Flood IO</a>を使い、ZeeMee Webサービスのステージング環境に対して多くの同時並行処理でそのスクリプトを走らせました。テストの初期段階では、Flood IOがすぐにHeroku上に作ったアプリに大きな負荷をかけました。いっぽう、後半へ行くにつれ、Herokuの能力よりもFlood IOの使い方がボトルネックになっているようでした（この件は以下で詳述します）。</p><p>テストを行う中で、私たちはRPM(リクエスト/分)、レスポンスタイム、dynoに対する負荷、データベースに対する負荷（CPU、IOPS、メモリ）を測定し、さらにNew RelicやHerokuのメトリクス機能を使って、サーバのインストルメンテーションを行いました。</p><p>テスト全体を通して、コードベースにほぼ変更は加えませんでしたが、唯一変更を必要としたのは、”Test 16の問題”に対処するときです。また、WebワーカがPostgresデータベースへの最大接続数を超えてしまった際に<a href="https://github.com/heroku/heroku-buildpack-pgbouncer" rel="nofollow" title="" class="ext-link">PgBouncer</a>を追加しました。それ以外は、テスト間でRailsのコードベースに手を加えていません。ですから、Heroku上で追加のハードウェアを加えたときに、任意のアプリがいかにスケーリングするかを見極めるには、今回の結果は適したプロキシと言えるでしょう。</p><p><img class="progressiveMedia-image js-progressiveMedia-image" data-src="https://cdn-images-1.medium.com/max/1200/1*d2Pq9ZBCL1iQudTpL62MMQ.png" src="https://cdn-images-1.medium.com/max/1200/1*d2Pq9ZBCL1iQudTpL62MMQ.png"><br>
様々なHeroku設定における結果概要</p><p><img class="progressiveMedia-image js-progressiveMedia-image" data-src="https://cdn-images-1.medium.com/max/1200/1*8LQmVgAXCBdLt-plof89Ug.png" src="https://cdn-images-1.medium.com/max/1200/1*8LQmVgAXCBdLt-plof89Ug.png"><br>
各テストによるRPM</p><p><img class="progressiveMedia-image js-progressiveMedia-image" data-src="https://cdn-images-1.medium.com/max/1200/1*YfBWEUW9ZHClPFladQsGhA.png" src="https://cdn-images-1.medium.com/max/1200/1*YfBWEUW9ZHClPFladQsGhA.png"><br>
各テストによるレスポンスタイム</p><p><strong>テスト1から5</strong>では、dyno数とデータベースハードウェアを増やしたので<strong>予想通り、スケールアップ</strong>しました。</p><p><strong>注目すべきはテスト6です。全く意味が分かりません！</strong>　dyno数を倍に増やしたのに、スループットが半減しています。恐らくこれは、2つのdynoが共存した他のHerokuユーザのdynoが非常に多忙だったのでしょう。ハイパーバイザが多忙であるにも関わらず、dynoのインスタンスは低いCPU稼働になっているようでした。この謎を解明するのは、内側からMatrixの外側を見るようなものです。ヒントはありましたが、どれも具体的なものではありませんでした。この件について突き詰めていくと、Herokuのサポートチームが私たちの疑念を解き明かしてくれました。</p><p>しかし、さらにひどいのは、ロードバランシング用にHerokuがランダムルーティングのアルゴリズムを使用しているため、一つ遅いdynoがあると全てのアプリを使い物にならなくしてしまうのです。単体のHerokuアプリで短いレスポンスタイムと長いレスポンスタイムを混在させると、アプリ全体のパフォーマンスがめちゃくちゃになることは<a href="https://www.google.co.jp/?gfe_rd=cr&amp;ei=RiECWMyRBqTf8Ae18pSgAg&amp;gws_rd=cr#q=heroku+performance+random+routing" rel="nofollow" title="" class="ext-link">よく知られています</a>。テスト結果における重要な点は、シングルテナント（performance）dynoを使わない限り、レスポンスが遅い状態をコントロールすることはできないとうことです。騒々しいご近所さんは、遅いクエリと速いクエリを混在させることで起きる同様の問題の原因になります。</p><p><strong>複数のテナントの問題を乗り越えたテスト7以降では、何も問題は起きませんでした。</strong>総合的にみて、私たちはHerokuのパフォーマンスに満足しています。しばらくすると効果は減少していったものの、ハードウェアの追加に比例して、大半のスケーリングはうまくいきました（<strong>テスト16は例外的ですが、特定のSQLクエリを向上させることで、これは簡単に対処できます</strong>）。</p><p>最終的に私たちは、HerokuそのものよりもむしろFlood IOの設定が弱点であるということに行き着きました。より多くの分散型のFlood設定でさらにテストを行い、その時がきたら結果を報告したいと思います。</p><p>最後に、Heroku上でRailアプリをスケーリングする際に覚えておいてほしいことを以下に挙げておきます。</p>
