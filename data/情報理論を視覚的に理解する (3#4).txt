<p><a href="http://colah.github.io/posts/2015-09-Visual-Information/" rel="nofollow" title="" class="ext-link">Visual Information Theory</a> （2015-10-14） by <a href="http://colah.github.io/about.html" rel="nofollow" title="" class="ext-link">Christopher Olah</a></p><p><script type="text/x-mathjax-config">MathJax.Hub.Config({ tex2jax: { inlineMath: [['$','$'], ['\\(','\\)'],['\\[','\\]']], processEscapes: true  },  CommonHTML: { matchFontHeight: false }});</script><br>
<script async src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_CHTML"></script></p><p>オーストラリアへ引っ越す直前、Bobは、私が作ったもう1人の人物、Aliceと結婚しました。私も、私の頭の中の他のキャラクターたちも驚いたことに、Aliceは犬好きではありませんでした。彼女は猫好きだったのです。それにもかかわらず、2人は、それぞれ動物に夢中で、非常に語彙が限られているという共通点を見出したのでした。</p><p><img src="http://colah.github.io/posts/2015-09-Visual-Information/img/DogCatWordFreq.png" alt="" data-disqus-identifier="/posts/2015-09-Visual-Information/disqussion-183"><br>
<em>注釈：犬好きが使う単語の頻度　猫好きが使う単語の頻度</em></p><p>2人は、同じ単語を使いますが、その頻度だけは異なります。Bobはいつも犬の話をしているし、Aliceは猫の話ばかりしています。</p><p>最初に、AliceがBobのコードを使ったメッセージを私に送ってきました。あいにく、そのメッセージは必要以上に長いものでした。Bobのコードは彼の確率分布に最適化されていました。Aliceは別の確率分布を持っており、Bobのコードを使うのは最適とは言えません。Bobが自身のコードを使う際のコードワードの平均的な長さは1.75ビットであるのに対し、Aliceが彼のコードを使うと平均2.25ビットになります。2人がそれほど似ていなければ、もっと悪い状況になっていたでしょう。</p><p>この長さ（ある分布から事象を交信する際に、他の分布に最適なコードを用いた場合の平均的な長さ）は、交差エントロピーと呼ばれます。交差エントロピーは、次の公式で定義できます。<sup id="fnref:4"><a href="#fn:4" class="footnote-ref">1</a></sup><br>
<span class="math">\[H_p(q) = \sum_x q(x)\log_2\left(\frac{1}{p(x)}\right)\]</span></p><p>この場合、犬好きのBobの単語の頻度に対する、猫好きのAliceの単語の頻度の交差エントロピーです。</p><p><img src="http://colah.github.io/posts/2015-09-Visual-Information/img/CrossEntropyDef.png" alt="" data-disqus-identifier="/posts/2015-09-Visual-Information/disqussion-184"><br>
<em>注釈：交差エントロピー： <span class="math">\(H_p(q)\)</span><br>
<span class="math">\(p(x)\)</span>のコードを使った<span class="math">\(q(x)\)</span>のメッセージの長さの平均</em></p><p>コミュニケーションのコストを下げるため、Aliceに自分のコードを使うよう頼みました。すると、彼女のメッセージの長さ平均が下がり、私はホッとしました。しかしこれは新たな問題をもたらしました。時折、Bobが誤ってAliceのコードを使ってしまうようになったのです。しかも驚くことに、Bobが間違ってAliceのコードを使うほうが、Aliceが彼のコードを使う場合よりも状況が悪化したのでした。</p><p>ここで考えられる可能性は4つです。</p><p>これは必ずしも、感覚的に分かるものではありません。たとえば、<span class="math">\(H_p(q) \neq H_q(p)\)</span>などです。では、以上4つの値の相互関係を表すにはどうすればよいでしょうか。</p><p>次の図表で、それぞれのサブプロットはこれら4つの可能性の1つを表しています。各サブプロットはメッセージの平均の長さを先の図表がしたのと同じように可視化しています。四角形で整理しているので、メッセージが同じ分布から来ている場合はプロットは隣同士に並び、同じコードを使っている場合は、上下に並びます。こうして、分布とコードを視覚的に一緒にスライドさせることができます。</p><p><img src="http://colah.github.io/posts/2015-09-Visual-Information/img/CrossEntropyCompare.png" alt="" data-disqus-identifier="/posts/2015-09-Visual-Information/disqussion-185"><br>
<span class="math">\(H_p(q) \neq H_q(p)\)</span>になる理由は分かりますか？<span class="math">\(H_q(p)\)</span>が大きいのは、<span class="math">\(p\)</span>下で非常によく起こるものの、<span class="math">\(q\)</span>下では稀なためコードが長い事象(青色）があるためです。一方で、<span class="math">\(q\)</span>下でよく起こる事象は<span class="math">\(p\)</span>下ではまれなものの、その差分は極端ではないので、<span class="math">\(H_p(q)\)</span>はそれほど高くないのです。</p><p>交差エントロピーは対称ではありません。</p><p>では、なぜ交差エントロピーを考慮しなければならないのでしょうか。それは、交差エントロピーが2つの確率分布がいかに異なっているかを表現する手段になるからです。<span class="math">\(p\)</span>と<span class="math">\(q\)</span>の分布が違っているほど、<span class="math">\(p\)</span>の<span class="math">\(q\)</span>に対する交差エントロピーは<span class="math">\(p\)</span>のエントロピーよりも大きくなるでしょう。</p><p><img src="http://colah.github.io/posts/2015-09-Visual-Information/img/CrossEntropyPQ.png" alt="" data-disqus-identifier="/posts/2015-09-Visual-Information/disqussion-186"><br>
同様に、<span class="math">\(p\)</span>が<span class="math">\(q\)</span>と異なれば異なるほど、<span class="math">\(q\)</span>の<span class="math">\(p\)</span>に対する交差エントロピーは<span class="math">\(q\)</span>のエントロピーよりも大きくなります。</p><p><img src="http://colah.github.io/posts/2015-09-Visual-Information/img/CrossEntropyQP.png" alt="" data-disqus-identifier="/posts/2015-09-Visual-Information/disqussion-187"><br>
とても面白いのは、エントロピーと交差エントロピーの差です。その違いは、別の分布用に最適化したコードを使ったことによって、どれほどメッセージが長くなるかを示しています。分布が同じであれば、この違いはゼロになります。差が広がるにつれ、長さも大きくなっていきます。</p><p>この差をKullback–Leibler情報量、あるいは単にKL情報量と呼びます。<span class="math">\(q\)</span>に対する<span class="math">\(p\)</span>のKL情報量、<span class="math">\(D_q(p)\)</span><sup id="fnref:5"><a href="#fn:5" class="footnote-ref">2</a></sup>は次のように定義されます<sup id="fnref:6"><a href="#fn:6" class="footnote-ref">3</a></sup>。</p><p><span class="math">\[D_q(p) = H_q(p) – H(p)\]</span><br>
KL情報量が非常に巧みなのは、それが2つの分布の間の距離のようであることです。KL情報量は分布の差異を測るのです（この考え方を突き詰めると、情報幾何学になります）。</p><p>交差エントロピーとKL情報量は、機械学習の分野で大変役に立ちます。ある分布を別の分布に近づけたい事例はよくあるからです。例えば、予測した分布を正解データに近づけられないかと考えることがあるでしょう。KL情報量を使えばそれが当然のように実現できるので、様々なところで利用されています。</p><p>先の天候と服装の例に戻りましょう。</p><p><img src="http://colah.github.io/posts/2015-09-Visual-Information/img/prob-2D-factored1-detail.png" alt="" data-disqus-identifier="/posts/2015-09-Visual-Information/disqussion-188"><br>
多くの親と同じく、母は私が天候にふさわしくないものを着ているのでは、と心配します（その疑いには理由があって、私はよく冬場にコートを忘れるのです）。ですから母は、天候と私が着るものの両方をしばしば知りたがるということです。これを伝えるためには、彼女に何ビット送ればよいでしょうか。</p><p>簡単に考えるには、確率分布を平坦化することです。</p><p><img src="http://colah.github.io/posts/2015-09-Visual-Information/img/prob-2D-factored1-flat.png" alt="" data-disqus-identifier="/posts/2015-09-Visual-Information/disqussion-189"><br>
これで、これら確率の事象に最適なコードワードが分かり、メッセージの長さの平均を計算できます。</p><p><img src="http://colah.github.io/posts/2015-09-Visual-Information/img/Hxy-flat.png" alt="" data-disqus-identifier="/posts/2015-09-Visual-Information/disqussion-190"><br>
これは、確率変数<span class="math">\(X\)</span>と<span class="math">\(Y\)</span>の結合エントロピーと呼ばれ、次のように定義されます。</p><p><span class="math">\[H(X,Y) = \sum_{x,y} p(x,y) \log_2\left(\frac{1}{p(x,y)}\right)\]</span></p><p>これは、1つではなく2つの変数があること以外は、通常の定義と全く同じです。</p><p>もう少しばかり良い考え方は、分布を均すのを避け、コードの長さを3次元として見ることです。エントロピーが体積になりました。</p><p><img src="http://colah.github.io/posts/2015-09-Visual-Information/img/Hxy-3D.png" alt="" data-disqus-identifier="/posts/2015-09-Visual-Information/disqussion-191"><br>
しかし、母が既に天候を知っているとしましょう。ニュースでチェックしたのです。この場合、与えるべき情報はどれほどでしょうか。</p><p>やはり、自分が着る服を伝達するため、多くの情報を送る必要があるように見えます。しかし、天候は私が着るであろう服装をはっきりと示唆するので、より少ない情報の送信で済むのです。雨の場合と晴れの場合を分けて考えてみましょう。</p><p><img src="http://colah.github.io/posts/2015-09-Visual-Information/img/HxCy-sep.png" alt="" data-disqus-identifier="/posts/2015-09-Visual-Information/disqussion-192"></p><p>雨でも晴れでも、概して多くの情報を送る必要はありません。なぜなら、天候が分かれば正解を推測しやすいからです。晴れの時は、晴れに最適化した特別なコードを使い、雨の時は雨に最適化したコードを使います。どちらの場合も、両方のための汎用コードを使うのに比べ、送る情報は少なくなります。母親に送る平均的な量の情報を取得するには、これら2つのケースを一緒にすればよいのです…。</p><p><img src="http://colah.github.io/posts/2015-09-Visual-Information/img/HxCy.png" alt="" data-disqus-identifier="/posts/2015-09-Visual-Information/disqussion-193"><br>
これを条件付きエントロピーと呼びます。公式に落とし込むと次のようになります。</p><p><span class="math">\[H(X|Y) = \sum_y p(y) \sum_x p(x|y) \log_2\left(\frac{1}{p(x|y)}\right)\]</span> <span class="math">\[~~~~ = \sum_{x,y} p(x,y) \log_2\left(\frac{1}{p(x|y)}\right)\]</span></p><p>前の章で、変数の1つを知っていれば、もう1つの変数はより少ない情報で伝達できることを見てきました。</p><p>これについて考える時に良い方法は、情報の量を帯としてイメージすることです。互いの間に共有する情報がある場合、それらの帯は重なり合います。例えば、<span class="math">\(X\)</span>と<span class="math">\(Y\)</span>が持つ情報に共有部分がある時、<span class="math">\(H(X)\)と<span class="math">\(H(Y)\)は重なり合うのです。そして、<span class="math">\(H(X,Y)\)</span>は両方に存在する情報なので、<span class="math">\(H(X)\)と<span class="math">\(H(Y)\)の帯の和集合です。<sup id="fnref:7"><a href="#fn:7" class="footnote-ref">4</a></sup></span></span></span></span></p><p><img src="http://colah.github.io/posts/2015-09-Visual-Information/img/Hxy-info-1.png" alt="" data-disqus-identifier="/posts/2015-09-Visual-Information/disqussion-194"><br>
上記のように考えると、多くのことが分かりやすくなります。</p><p>例えば、「<span class="math">(X)</span>と<span class="math">(Y)</span>の両方を伝える場合（”結合エントロピー”:<span class="math">\(H(X,Y)\)</span>）に必要な情報の量は、<span class="math">\(X\)</span>のみを伝える場合（”周辺エントロピー”:<span class="math">\(H(X)\)）よりも多くなる」ということを前述しました。しかし、既に<span class="math">\(Y\)</span>を知っているなら、それを知らない場合よりも少ない情報で<span class="math">\(X\)</span>を伝えることができます（”条件付きエントロピー”:<span class="math">\(H(X|Y)\)</span>）。</span></p><p><img src="http://colah.github.io/posts/2015-09-Visual-Information/img/Hxy-overview.png" alt="" data-disqus-identifier="/posts/2015-09-Visual-Information/disqussion-195"><br>
少々複雑に思えるかもしれませんが、帯のイメージで考えるととてもシンプルです。<span class="math">\(H(X|Y)\)</span>は、既に<span class="math">\(Y\)</span>を知っている誰かに<span class="math">\(X\)</span>を伝える時に送らなければならない情報（<span class="math">\(X\)</span>に含まれる情報のうち、<span class="math">\(Y\)</span>に含まれない情報）です。視覚的に言えば、<span class="math">\(H(X|Y)\)</span>は<span class="math">\(H(X)\)</span>帯のうち<span class="math">\(H(Y)\)</span>と重なり合わない部分です。</p><p>これで、次の図表から不等式<span class="math">\(H(X,Y) \geq H(X) \geq H(X|Y)\)</span>を読み取ることができるでしょう。</p><p><img src="http://colah.github.io/posts/2015-09-Visual-Information/img/Hxy-info-4.png" alt="" data-disqus-identifier="/posts/2015-09-Visual-Information/disqussion-196"><br>
もう1つの式は<span class="math">\(H(X,Y) = H(Y) + H(X|Y)\)</span>です。<span class="math">\(X\)</span>と<span class="math">\(Y\)</span>の情報は、<span class="math">\(Y\)</span>の情報に、<span class="math">\(X\)</span>に含まれているものの<span class="math">\(Y\)</span>には含まれていない情報を加えた和と等しいということです。</p><p><img src="http://colah.github.io/posts/2015-09-Visual-Information/img/Hxy-overview-sum.png" alt="" data-disqus-identifier="/posts/2015-09-Visual-Information/disqussion-197"><br>
これも、等式で見ると難しいのですが、重なり合う情報の帯として考えれば簡単です。</p><p>ここでは、<span class="math">\(X\)</span>と<span class="math">\(Y\)</span>の情報をいくつかの方法で分割しています。各変数に関する情報<span class="math">\(H(X)\)</span>と<span class="math">\(H(Y)\)</span>があります。その両方の和集合、<span class="math">\(H(X,Y)\)</span>があります。片方にあって片方にない情報、<span class="math">\(H(X|Y)\)</span>と<span class="math">\(H(Y|X)\)</span>も持っています。この多くが、変数間で共通している情報、つまり情報の積集合を中心に展開しているように見えます。この積集合を相互情報量、<span class="math">\(I(X,Y)\)</span>と呼び、次のように定義し  ます。:<sup id="fnref:8"><a href="#fn:8" class="footnote-ref">5</a></sup><br>
<span class="math">\[I(X,Y) = H(X) + H(Y) – H(X,Y)\]</span><br>
この定義が機能するのは、<span class="math">\(X\)</span>と<span class="math">\(Y\)</span>の両方それぞれに相互情報量が1セットずつ含まれる故に<span class="math">\(H(X) + H(Y)\)</span>には相互情報量が2セット含まれる一方で、<span class="math">\(H(X,Y)\)</span>には相互情報量が1セットしか含まれないからです（前の帯の図表を思い出してください）。</p><p>相互情報量と密接に関わっているのは、情報偏差です。情報偏差は変数間で共有されていない情報です。以下のように定義できるでしょう。<span class="math">\[V(X,Y) = H(X,Y) – I(X,Y)\]</span><br>
情報偏差が面白いのは、異なる変数間の測定基準として距離を提供してくれるからです。「1つの変数の値が分かればもう1つの変数の値もわかる」という場合は情報偏差はゼロになり、2つの変数が独立になるにつれて2つの変数間の情報偏差は大きくなります。</p><p>これは、同様に距離感を示すKL情報量とどのように関わっているでしょうか。KL情報量で分かるのは、同じ変数や変数の組における2つの分布間の距離です。対して情報偏差は、連帯して分布する2つの変数の間の距離を表します。KL情報量は複数の分布の間の関係であり、情報偏差は1つの分布の中にあるのです。</p><p>以上すべてを1つの図表にまとめ、様々な種類の情報の関係性が分かるようにしましょう。</p><p><img src="http://colah.github.io/posts/2015-09-Visual-Information/img/Hxy-info.png" alt="" data-disqus-identifier="/posts/2015-09-Visual-Information/disqussion-198"></p>
