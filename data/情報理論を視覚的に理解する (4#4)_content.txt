Visual Information Theory （2015-10-14） by Christopher OlahMathJax.Hub.Config({ tex2jax: { inlineMath: [['$','$'], ['\\(','\\)'],['\\[','\\]']], processEscapes: true  },  CommonHTML: { matchFontHeight: false }});
情報理論の非直感的な点として、小数のビットがあるということが挙げられます。少しおかしな気もしますが、例えば0.5ビットの存在にどんな意味があるのでしょうか。簡単な答えは次のとおりです。多くの場合、私たちが重視するのは特定のメッセージの長さではなくメッセージの平均的な長さです。送信メッセージの半分が1ビットで残りの半分が2ビットの場合、平均すると1.5ビットになります。こうして見ると、平均値が小数を含むのはおかしなことでも何でもありません。しかし実際には、その答えは問題の核心を避けています。多くの場合、コードワードの最適な長さは小数ですが、それはどういうことなのでしょうか。具体的に説明します。あるイベント\(a\)が71％の確率で発生し、\(b\)が29％の確率で発生するような確率分布を考えてみてください。
\(a\)を表すのに最適なコードは0.5ビットで、\(b\)は1.7ビットとなります。これらのコードワードを単一で送信するのは単純に無理な話で、送る場合はビットを整数に丸めて平均1ビットにしなければなりません。しかし、複数のメッセージを同時に送る場合、もっと効率をよくすることが可能です。この分布から2つのイベントを送ることを考えてみましょう。個々のイベントを別々に送ると2ビットが必要になりますが、もっといい方法はあるでしょうか。
送る頻度は\(aa\)が50％、\(ab\)と\(ba\)が21％、そして\(bb\)が8％です。ここでも、理想的なコードは大半が小数のビットです。
コードワード長を丸めると、次のようになります。
コードの平均メッセージ長は1.8ビットです。個別に送った時の2ビットよりも少ない結果となりました。別の見方で考えると、それぞれのイベントで平均して0.9ビットを送っていることになります。もし同時に送るイベント数が増えれば、ビット数はまだ小さくなるでしょう。\(n\)が無限大に近づくほど、コードを丸めることに起因するオーバーヘッドはゼロに近づき、コードワード当たりのビット数はエントロピーに近づきます。さらに、\(a\)の理想的なコードワード長は0.5ビットで、\(aa\)の理想的なコードワード長は1ビットでした。理想的なコードワードは小数のビットであっても加算されます。つまり、多くのイベントを同時に送れば、長さは増えるのです。小数の情報ビットがあるということには、たとえ実際のコードが整数しか扱えないとしても、現実的な意味があることが分かっていただけたでしょうか。（実際に一般で使われているのは、異なる領域で効果的な特定の符号スキームです。ハフマン符号は、基本的にここで述べたのと同じタイプのコードですが、小数ビットの扱いに長けてはおらず、エントロピー限界に近づくためには、上で行ったようにシンボルをグループ化するか、もっと複雑なトリックを使わなくてはなりません。これに対して算術符号は小数ビットをうまく扱い、徐々に最適になるようにします。）最小ビットで送りたいと思う場合、上記の考えは基本的なものです。データの圧縮を気に掛けるなら、情報理論はその問題の核心に迫ることができますし、それにより基礎的で正しい抽象化が得られます。しかし、そうでない場合、つまりデータの圧縮を気にするとかでない場合―情報理論とは好奇心以上のものにはならないのでしょうか？情報理論を元にしたアイデアは、例えば機械学習、量子力学、遺伝学、熱力学、さらにはギャンブルまで、多くの文脈において見ることができます。しかし、これらの分野の人たちは、通常、情報を圧縮したいという理由で情報理論に関心を持つことはありません。彼らが情報理論に関心を持つのは、情報理論が彼らの活動分野に大きな関係を持っているからです。量子もつれは、エントロピーで説明することができます。1統計力学と熱力学の多くの結果は、未知のことについて最大エントロピーを仮定することによって導き出すことが可能です。2ギャンブラーの勝敗はカルバック・ライブラー情報量、特に反復的な想定でのケースに直接的な関係があります。3こうした多くの分野で活用されている理由は、私たちが表現したいと思う多くの事物に対して、情報理論が具体的で原理的な形式を提供しているからです。情報理論によって、私たちは「不確定性」「2つの信念がどのように異なっているか」「ある問いに対する一つの答えにより、別の問いについて何がわかるか」などを測定・表現する方法を得ることができるのです。これらはつまり、確率の散漫さや確率分布間の距離、2つの変数の依存度などです。こうしたことを実現できる方法が他にあるでしょうか。もちろんあることはあるでしょう。しかし情報理論の考え方は明快で、その性質は良好であり原則に基づいた起源を有しています。そして、そこから導き出されるものは、ある場合には私たちがまさに関心を持っていたことであり、別の場合には、この乱雑な世界の中での有効な代理の尺度なのです。私の一番得意な分野は機械学習なので、それに関連して少しお話します。機械学習における一般的なタスクは分類です。例えば写真を見て犬か猫を予測するとしましょう。この予測において、私たちのモデルが「画像は80％の割合で犬、20％の割合で猫」と答えたとします。正しいのは犬だったとして、80％の確率とはどの程度、いい答えだったのでしょうか。また、85％と答えていた場合、それは80％の時に比べどの程度、優れていたのでしょうか。この問題は非常に重要です。なぜならモデルを最適化するためには、その善し悪しを示す見解が必要だからです。では、一体何を最適化すべきなのでしょうか。正しい答えは、そのモデルが使われる状況（最多の推測が正しければいいのか、正確な答えの確率が重要なのか、明確な間違いがどれほど好ましくないのか）に大きく依存するため1つではありません。あるいはほとんどの場合、正しい答えさえ知ることはできないでしょう。というのも対象の関心事を形式化するために、そのモデルがどのくらい適切な形で使われるかを私たちは知ることができないからです。その結果、交差エントロピーこそが一番の関心事になるような状況が生じてきますが、必ずしもそういった状況だけとは限りません。もっと言えば、何が関心事であるかを確実に対象化できないことも多々あり、そういった場合にも交差エントロピーは代理の尺度になります。4情報は、私たちが世界を考えるための新しい強力なフレームワークを提供してくれます。それが時には直面した問題を完全に解決することもあるし、完全ではないにしろ、有効に機能する時もあるでしょう。この記事では、情報理論の表面を軽くなぞっただけで、主要なテーマである誤り訂正符号などについては触れていませんが、情報理論というものが、特に身構える必要のない素晴らしい題材であることが分かっていただければ幸いです。今後、記事を書く時の参考にさせていただきたいので、ぜひフィードバックのフォームに記入してください。Claude Shannonが書いた情報理論についての最初の論文、「A Mathematical Theory of Communication（通信の数学的理論）」は非常に分かりやすいです（これは初期の情報理論の論文における反復パターンのようです。時代なのかページ数の不足なのか、それともベル研究所の文化なのかは分かりません）。Cover &amp; Thomasの「Elements of Information Theory（情報理論要素）」は標準的な参考文献で、私には非常に参考になりました。この記事に対して多くの時間を割いて信じられないほど詳細かつ広範なコメントを提供してくれたDan Mané、David Andersen、Emma Pierson、Dario Amodeiに感謝の意を表します。同じくMichael Nielsen、Greg Corrado、Yoshua Bengio、Aaron Courville、Nick Beckstead、Jon Shlens、Andrew Dai、Christian Howard、Martin Wattenbergのコメントにも感謝します。この記事のアイデアは、私が最初に担当したニューラルネットワークの2つのセミナーシリーズが試験台として機能することでより強固なものとなりました。最後に誤字脱字を指摘してくださった読者の皆さん、特にConnor Zwick、Kai Arulkumaran、Jonathan Heusser、Otavio Good、そして匿名でコメントをくれた方に感謝します。
注釈：
Understanding Convolutions（畳み込みを理解する）
Groups &amp; Group Convolutions（グループとグループの畳み込み）
Neural Networks, Manifolds, and Topology（ニューラルネットワーク、マニホールド、およびトポロジー）
Visualizing Representations（表現の可視化）
Deep Learning and Human Beings（深いレベルの学習と人間）
