Grepping log is terrible（2015-05-05）今でも、systemdのjournalにおけるバイナリのストレージフォーマットに関して、不満を漏らす人が多くいることに私は驚きを隠せません。私は長年、システム管理者として働いてきており、1年以上もsyslog-ngのオープンソースエディションのメンテナとして活動してきました。だからこそ、テキストではないストレージフォーマットに対して、なぜ多くの人が批判的なのか、私は理解に苦しんでいます。更に、反論を唱える人までいることが信じられません。もしかしたら、私は別世界の人間なのかもしれません。ですが、より良い選択肢があるのに、テキストのストレージを使う理由はほとんどありません。ロギングをする必要性、そしてなぜ、テキストのログストレージに対してそこまで用心深いのかについて、私は何度も尋ねられました。ここに、私が導き出した答えを紹介したいと思います。これは、journalについて弁明するものではないということをご承知おきください。私は、journalが導入される、かなり前からバイナリのログストレージをデプロイしてきましたし、私がここで推奨する解決策もjournalではありません。（正直なところ、journalのアイデアそのものは好きです。私が所有するいくつかのPCに搭載されているjournalの機能も楽しんでいます。しかし、それは私が必要とする解決策ではありませんし、将来的に必要とすることもないでしょう。）（なお、頂いたいくつかの反響にお答えして、フォローアップの記事を投稿しました。）まず初めに、システムと必要条件について説明します。異なるニーズに合わせた2つのセットアップがあります。私自身、以前はそれぞれ別の解決策を取っていましたが、後に1つに絞っています。まず1つ目は簡単なもの、つまりサーバ、コンピュータ、ガジェットを用いた個人的なセットアップです。このシステムは、VPS用マシンが2台、自宅にあるPCが1台、ノートPCが2台（どれもDebianが搭載されています）、Raspberry Pi Bが1台（ログの収集はしているかどうかもわからないOpenELEC、または、上記のインフラストラクチャに連結したRaspbianを搭載）で構成されています。以前はカスタムファームウェアを搭載したルータを所持していましたが、手放してしまいました。入力メッセージの数は、平均して毎秒数百件です。必要とする条件はシンプルです。より複雑なセットアップでは、システムも少し大きくなります。その主要部分は、処理能力の高いマシンの5つのノードからなるクラスタであり、稼働しているメインのアプリケーショングループから、毎日、各ノード100ギガバイトの生ログを収集します。こちらの要件も小規模な構成の場合とほぼ同様です。小規模なシステムでは、テキストでログを保存する場合に2つの問題が挙げられます。大規模なシステムでは、テキストでログを保存する場合、上で挙げたことに加えて更なる問題があります。セッションごとに分ける場合は、あるユーザの全てのログを見つけるために多数のファイルを確認しなければいけません。そして、この場合も時間やサイズによる分割が必要です。セッションは多量のログを発生させるため、1つのファイル内に保存するのは非効率的です。ログを複製し、異なる種類のクエリに対応するように違った方法で保存すると、あっという間にディスクの容量がなくなってしまいます。これは、やはり非効率的です。もっといい方法があるはずです。大規模システムの詳細はお話しできませんが、私が個人的に実践しているログの、使い方をいくつかご紹介しましょう。あまり難しいクエリではありませんが、これが次のクエリの基本になります。まず、この説は正しくありません。もし、バイナリログが破損しても、append-only(追記のみ)であれば有効な部分を取り出すことができます。確かに、このためのツールが必要になりますが、大きな問題ではないでしょう。バイナリログの優れているところは、損傷するとすぐに分かる点です。また、検証や暗号による署名なども容易です。テキストログにも損傷する可能性はあります。私は何度も経験してきました。バイナリログの有効な部分を復旧するのは、ちっとも難しくありません。その通りです。しかし、その何が問題なのでしょう。テキストログにもツールは必要です。grepやlessなど、皆さんが使いたいものを使ってください。また、grepを使うためにログのフォーマットについて確認する必要があります。バイナリログでは、検索しやすい方法で構造化データを保存するのがより簡単になります。特定のメールに関するログが必要な場合は、message-idフィールドを探すことが可能で、これによって取得できます。テキストログでは、message-idフィールドの記述にのみ一致する正規表現を構築しなければなりません。できないことはありませんが、少し手間ですね。切り替え困難な解決策を利用しているなら、その通りです。切り替える必要はありません。そうですね。それはwtmpやutmpのようにでしょうか？　または、全てのデータベースのようにでしょうか？テキストが有効なのは、マシンが1つで、多数の独立したアプリケーションがあり、それを異なるログファイルに分けることができる場合です。しかし、システム全体が対象で、相互依存するアプリケーションがある場合は、アドホッククエリを得るために、ログをインデックス化しなければいけません。その通りです。しかし、だからどうだというのでしょう。ログをクエリするのに特別なツールが必要なことに変わりはありません。バイナリストレージの利点を享受してみるのもいいでしょう。この説には賛同できません。ログをSQLに保存するのが主流になってから10年以上が経ち、これが衰退することはないように思えます。非リレーショナルデータベースと検索エンジンの登場で、SQLでログを扱うようになったばかりの時代より、ログの保存や検索のツールは改善されています。テキストログと大差はありません。テキストログはヒューマンリーダブルな形式になっていますが、読み方が分からなければ役に立ちません。アプリケーションの正確なロギングフォーマットを知っていればログを理解できますが、読み方が分からなければ、それは単語と記号の羅列でしかありません。例えば以下のログで、どのフィールドがリファラか分かりますか？皆さんがどうかは分かりませんが、私はHTTP/1.1″ \d+ \d+ “-“よりもreferrer=”-“使う方が好きです。もちろん、ログを事前に処理してJSONなどに保存し、jq ‘.referer=”-“‘を使用することもできます。しかし、この方法だと大規模なログファイルを扱う場合に、処理速度がとても遅くなってしまいます。また、JSONもヒューマンリーダブルな形式ではないので、JSON内を検索する場合も特別なツールが必要になります。最も頻繁に耳にするのは、以下のようなことです。もしも1日に100ギガバイトのログがあるなら、何か間違ったことをしていますし、ログを多く取り過ぎています。失礼ですが、私たちがどのくらい、何をログすると決めるかは、あなたには関係のないことです。自分で決定しますし、私たちはそれだけの量を扱わなくてはいけないのです。grepしやすいように、ログを小さなファイルに分ければいいのです。はい、違いますね。ファイルサイズは問題ではありません。問題は、データの量です。grepするデータの量に制限をかける必要があるでしょう。何百ギガバイトという単位では大きすぎるからです。特に、反復して、検索を絞り込む（単に狭めるのではなく、おそらく拡張もして）場合にはそうです。先に挙げた大規模な構成の要件を見てください。ファイルを分割する合理的な方法がないので、私たちにはインデックスが必要です。正規表現を正確に学びなさい。おやまあ、どこから手をつけたらいいのでしょう？正規表現はテキストのマッチングには最適なのですが、私のデータはテキストではなく構造化された値のセットです。ですから私は、基本的には正規表現の記述も解析しなくてはならなくなります。あるいはgrepのように使え、かつより便利な、なにか特殊なツールが必要でしょう。どちらにしても反論がありそうです。率直に言わせてもらえば、2行の正規表現は、ヒューマンリーダブルではありません。そして2行というのは、私たちが行う（私たちが実際に試した）いくつかの検索のために必要になるであろう正規表現の標準的な長さです。検索を複数の段階に分割することで正規表現を簡単にすることができますが、それを行うと応答時間やリソースの使用に深刻な影響がありますから、そうはしないでしょう。いくつかのシステムは関連づけられていないので、それぞれで検索エンジンを動作させたりログにインデックスをつけたりすることは莫大な時間を無駄にすることになります。その通りです。そしてテキスト形式のログストレージでもまったく問題ありません。それがあなたのものであれば。私なら、それでもバイナリログストレージを使います。なぜならバイナリログは書いたり解析したりするのにより効果的だということが分かったからです。しかしこの場合、インデックスすることは役に立ちません。バイナリログを書くことができるsyslogは、できないものに比べても大きさは変わらず、更なるリソースを必要とすることもありません。より多くを必要とするのはログにアクセスするためのツールですが、このような小さいデータならlogcatプログラムが必要になるだけです。それはおそらく、lessでなければ15～20kBくらいでしょう。自身のログを保存するほとんどのデバイスでは、これはさほど大きくはありません。もしも大きいなら、そのデバイスにログを保存するべきではないでしょう。いえ、簡単にはなりません。lessを使ってログのブラウジングをするのは信じられないほど退屈ですし、非常に困難なプロセスです。非常に多くのログがある場合は特にそうでしょう。それに、バイナリログのブラウジングはちっとも大変ではありません。lessの代わりにlogcatや、それに似たものを使うでしょう。tail -fを使っているかのように追加されたキューを監視できますし、何だってできます。もしかしたら、また違った名前のツールを使う必要があるかもしれませんね。まぁ、なんと大きな問題でしょうか！私がテキストベースのログストレージを使うのをやめてからのここ数年、生のテキストログの必要性を感じたことは、正確に数えると…一度もありません。私が利用している検索とブラウズのツールは、grepやlessの10倍は便利です。私は自分のログの構造を覚えておく必要がありませんし、その知識を全ての正規表現に応用する必要がありません。私のツールははるかに賢いのです。アラートに加えるべきイベントを探すのも簡単になりましたし、傾向を見るのも、持っているデータを理解するのも簡単になりました。なぜでしょうか？　それは、私のためにコンピュータに仕事をさせ、それをより効率的にし、コンピュータが対処できるフォーマットでログを保存するからです。ともかく、私には必要がないので、生データは見ません。私は怠け者なので、コンピュータが仕事をしてくれるなら、その恩恵を享受したいのです。やらなくて済むなら、正規表現を書くために指さえ持ち上げたくありません。その代わりに、より理解しやすいクエリ言語を求めます。私と、テキストベースのログストレージの信奉者の主な違いを挙げますと、私は結果がヒューマンリーダブルであるクエリを求め、それがどのように作られるのかを気にしません。一方、信奉者たちはヒューマンリーダブルな生データを求め、データをテキスト形式で保存するために利便性を犠牲にするのです。申し訳ありませんが、私は信条よりも利便性と効率を重視します。
