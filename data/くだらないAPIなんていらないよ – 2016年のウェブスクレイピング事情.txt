<p><a href="https://franciskim.co/2016/dont-need-no-stinking-api-web-scraping-2016-beyond/">I Don't Need No Stinking API ? Web Scraping in 2016 and Beyond</a> by <a href="https://franciskim.co/">Francis Kim</a></p><p>ソーシャルメディアのAPIとそのレート制限は、あまり気分のよいものではありません。特にInstagram。あんな制限つきAPIを欲しがる人がいったいどこにいるんでしょうね？</p><p>最近のサイトは、スクレイピングやデータマイニングの試みを阻止するのがうまくなってきました。AngelListはPhantomJSすら検出してしまいます（今のところ、他のサイトでそこまでの例は見ていません）。でも、ブラウザ経由での正確なアクションを自動化できたとしたら、サイト側はそれをブロックできるでしょうか？</p><p>並行性を考えたり、さんざん苦労して用意した結果として得られるものを考えたりすると、Seleniumなんて最悪です。あれは、私たちが「スクレイピング」と聞いて思い浮かべるようなことをするためには作られていません。しかし、賢く作り込まれた今どきのサイトを相手にして、インターネットからデータを掘り当てるための信頼できる方法はといえば、ブラウザの自動化だけなのですよねえ。</p><p>私の持ちネタは全部JavaScriptです。あまり読者を稼げないかもしれませんね😑😆。WebdriverIO、Node.js、そして<a href="https://www.npmjs.com/package/antigate" rel="nofollow" title="" class="ext-link">antigate</a>（参考：<a href="https://www.troyhunt.com/breaking-captcha-with-automated-humans/" rel="nofollow" title="" class="ext-link">Troy Hunt – Breaking CAPTCHA with automated humans</a>）などの大量のNPMパッケージ。全部JavaScriptですが、今回紹介するテクニックの大半は、どんなSelenium 2ドライバにも適用できます。たまたま私は、ブラウザの自動化にはJavaScriptが向いていると感じただけのことです。</p><p>Seleniumを使った自動化／スクレイピング／クローリングについて、これまで学んできたすべてのことをみなさんと共有しようと思います。この投稿の目的は、私が編み出したテクニックの中でまだ公開していないものについて説明することです。さまざまな範囲に適用できるアイデアであり、ウェブ開発コミュニティ全体で共有して議論できるものだと思います。<del>あと、<a href="http://thehustle.co/kobe-bryant-venture-capital" rel="nofollow" title="" class="ext-link">コービー・ブライアントに出資してもらいたい</a>という気持ちもあります</del>。</p><p><a href="https://news.ycombinator.com/item?id=12348860" rel="nofollow" title="" class="ext-link">Hacker Newsでコメントした内容を引用します</a>。</p><p>倫理的に考えて、私は大量のデータをスクレイピングしたりはしません。仕事としてスクレイピングをしたことはないし、そんな仕事でお金を稼ぐつもりはありません。</p><p>私にとってのスクレイピングは単に個人的な用途のためだけのものであって、ちょっとしたサイドプロジェクトでしか使いません。そもそも「スクレイピング」という言いかたが気に入らないんですよね。なんだかネガティブな意味にとられがちだし（このコメントのスレッドを見ても明らかですよね）、これってマーケティングの視点からの言葉じゃないですか。お手軽なものを求める人たちはスパムに走ります。テクノロジーの使い方が間違っているんです。</p><p>私はどちらかというと「自動化」とか「ボットを作る」という言いかたのほうが好みですね。私は単に、日々の生活を自動化しているだけだし、日々の作業（そう、たとえばFacebookを巡回して気に入ったgifや動画を自分のサイトで紹介したりとか）を肩代わりしてくれるボットを作っているだけです。毎日一時間もかけてそれを手動でやりたいと思いますか？私はそんなにマメじゃありませんね。</p><p>自分の生活のどこを自動化できてどこは自動化できないのか、教えてくれる人なんていないでしょう？</p><p>なら、やるだけのことですよ。</p><p>こんなふうにところどころでランダムな待ち時間を入れておくと、人間らしくなって安全です。</p><p>次のコードは、Facebookページから動画を取得する関数の一部です。</p><p>これとよく似たこんなメソッドと正規表現を組み合わせれば、コマンドラインのcURLライクなスクリプトでは読めないようなRSSフィードもパースできます。</p><p>これは<em>ほんとうに</em>うまく動いてくれます。いくつかのソースで試したけどまったく問題はありませんでした。</p><p>私くらいのクラスになると、クライアントサイドのJavaScriptを送り込むのもどうってことありませんね。</p><p>ところで、うすうす感づいてらっしゃるでしょうけど、このサンプルはまったくもって現実的じゃありません。次、行ってみましょう。</p><p><strong><a href="http://gifly.co/" rel="nofollow" title="" class="ext-link">GIFLY.coが48時間以上も更新されなかった</a></strong>ことがありました。FacebookページからアニメーションGIFを取ってくる私のスクリプトが、CAPTCHA画面でストップしていたのです😮。</p><p>FacebookのCAPTCHAを破るのはきわめて簡単で、15分もあれば突破できました。内部的になんとかしてしまう手もきっとあるのでしょうが、<a href="https://www.npmjs.com/package/antigate" rel="nofollow" title="" class="ext-link">NPMのAntigateパッケージ</a>を使えば低コストで実現できるし、何も考えることはありませんでした。</p><p>JavaScriptを差し込むのも朝飯前ですね。画像をキャンバスに変換してから、<code>.toDataURL()</code>を実行してBase64エンコードさらたPNG画像を取得します。そしてそれをAntigateのエンドポイントに送ります。この関数は、あるサイトから拝借しました。これ以外にもいろんなネタをいただいています。<a href="https://davidwalsh.name/convert-canvas-image" rel="nofollow" title="" class="ext-link">ありがとう、David Walsh</a>。このコードはFacebookのCAPTCHAを解読し、その値を入力してボタンをクリックします。</p><p>なぜクライアントサイドのAJAXのエラーを捕まえる必要があるのでしょう？それにはちゃんと理由があります。私はかつて、Instagramでフォローしているアカウントを<em>全部</em>アンフォローする処理を自動化しました。そのときに気づいたのですが、この処理にはレート制限があるようなのです。これは、API経由の場合だけではなく通常のWebインターフェイス経由でも同じでした。</p><p>フォロー／アンフォローはAJAXコールを伴い、レート制限を越えてしまうとAJAXのエラーが発生します。そこで私は、AJAXのエラーを横取りする関数を作ってグローバル変数に保存するようにしました。</p><p>アンフォロー処理後の値を毎回収集して、エラーが3回発生したらスクリプトを終了させます。</p><p>Instagramのスクレイピング（というかクローリングというかスパイダリングというか……）を行っているときに、問題が発生しました。タグのページのDOMに、投稿日が含まれていないのです。<strong><a href="http://iqta.gs/" rel="nofollow" title="" class="ext-link">IQta.gsで使いたいのでこのデータは必須だった</a></strong>のですが、毎回200枚の写真を処理しているので、すべての投稿にアクセスする余裕はありませんでした。</p><p>でもよく見ると、ブラウザが受け取るpostオブジェクトには<code>date</code>という変数が含まれていました。私はこの変数を使わずに、こんなふうに対応しました。</p><p>まあこれは、私の住むメルボルンではもはやちょっと時代遅れですね。</p><p>私はこれらをすべて、AWS上のDockerコンテナで動かしています。ちょっとしたbashスクリプトを書いて、Instagramクローラーの耐障害性を高めました<em>（いまちゃんと動いているか確認してみましょう）</em>。</p><p>うん。6つの<a href="http://iqta.gs/" rel="nofollow" title="" class="ext-link">IQta.gs</a>クローラーがきちんと動いているようですね 🙂 。Dockerを使っているときにちょっとした問題も発生しました。イメージが使えなくなったのです。理由はわかりません。根本原因を突き止めるのはあきらめましたが、私の書いたbashスクリプトはイメージがアクティブではなくなったことを検出できるようにしました。検出したら、イメージを削除したうえで、まっさらなイメージからSeleniumグリッドを再起動するように仕込んであります。</p><p>この記事を書き始める前からこの見出しだけは書いていたのですが、今となっては何を書くつもりだったのかを思い出せません。まあ明日になれば思い出すでしょう。</p><p>ああ、<a href="https://blog.hartleybrody.com/web-scraping/" rel="nofollow" title="" class="ext-link">Hartley Brodyの記事</a>は紹介しておかないといけませんね。Hacker Newsで2012年から2013年にかけて人気だった記事で、私が今回の記事を書くきっかけにもなりました。</p><p>「ていうか<code>browser</code>って何なのさ」と思った人向けに、その正体を書いておきましょう。</p><p><code>argv</code>は<a href="https://www.npmjs.com/package/yargs" rel="nofollow" title="" class="ext-link">yargs</a>から取得しています。</p><p>長年の友人であるGoogleのIn-Hoにこの記事をレビューしてもらいました。ありがとう！<a href="https://soundcloud.com/yiinho" rel="nofollow" title="" class="ext-link">彼の曲</a>もぜひ聴いてみてくださいね。</p>
