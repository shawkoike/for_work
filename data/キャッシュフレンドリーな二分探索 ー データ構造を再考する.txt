<p><a href="http://bannalia.blogspot.jp/2015/06/cache-friendly-binary-search.html" rel="nofollow" title="" class="ext-link">Cache-friendly binary search</a> （2015-06-25） by <a href="https://www.blogger.com/profile/08579853272674211100" rel="nofollow" title="" class="ext-link">Joaquín M López Muñoz</a></p><p>現代のコンピュータのアーキテクチャに搭載されている高速のキャッシュメモリは、<em>参照の局所性</em>に優れた(＝一連のものとしてアクセスした要素が、互いに近いメモリのアドレスに配置されている)データ構造を好みます。これは、<a href="http://www.boost.org/doc/libs/1_58_0/doc/html/container/non_standard_containers.html#container.non_standard_containers.flat_xxx" rel="nofollow" title="" class="ext-link">Boost.Containerの平坦な（ツリー状ではない）連想コンテナ</a>のようなクラスを陰で支えている理論的根拠です。要素を連続的に（かつ順序だてて）保存すると同時に、標準的なC++ノードベースの連想コンテナの機能性をエミュレートします。以下にあるのは、要素が0から30の範囲の時、<code>boost::container::flat_set</code>の中で<a href="https://ja.wikipedia.org/wiki/%E4%BA%8C%E5%88%86%E6%8E%A2%E7%B4%A2" rel="nofollow" title="" class="ext-link">二分探索</a>がどのように行われるのかを示した例です。</p><p><img border="0" height="48" src="http://3.bp.blogspot.com/-g7a7m_42A5w/VYs05n5fWmI/AAAAAAAABNY/CSxDV5K5Ujc/s1600/flat_set.png" width="400"><br>
探索で目的の値を絞り込むにつれて、アクセスされる要素は次第に近くなっていきます。そのため、最初のうちは大きな距離を飛び越えていくような感じであっても、参照の局所性は<em>このプロセスの最後の部分では</em>非常に良くなっています。概して、このプロセスは基本の<code>std::set</code>（アロケータの気まぐれで、要素はメモリのあちこちに散らばっています）での検索よりもキャッシュフレンドリーで、結果として探索時間もより改善されます。しかし、それよりもさらにキャッシュフレンドリーな方法があります。</p><p>0から30までの値を持つ、古典的な<a href="https://ja.wikipedia.org/wiki/%E8%B5%A4%E9%BB%92%E6%9C%A8" rel="nofollow" title="" class="ext-link">赤黒木</a>の構造を考えてみてください。</p><p><img border="0" height="122" src="http://1.bp.blogspot.com/-OeUzPe7-O_4/VYsSqDJZV0I/AAAAAAAABMQ/IZfwXIqcogg/s1600/rb_tree.png" width="400"><br>
そしてその要素を、次のような木の<a href="https://ja.wikipedia.org/wiki/%E6%9C%A8%E6%A7%8B%E9%80%A0_(%E3%83%87%E3%83%BC%E3%82%BF%E6%A7%8B%E9%80%A0)#.E5.B9.85.E5.84.AA.E5.85.88.E6.8E.A2.E7.B4.A2" rel="nofollow" title="" class="ext-link">幅優先</a>（レベル順）の連続した配列に並べます。</p><p><img border="0" height="15" src="http://1.bp.blogspot.com/-9gD_pbaWDyA/VYsTasCqpiI/AAAAAAAABMY/vwCuXYvHF_c/s1600/preorder_vector.png" width="400"><br>
この<em>レベル順ベクトル</em>では、次の例が示すように、最初にある要素（根）から二分探索を始め、”左”あるいは”右”の子にジャンプしていきます。</p><p><img border="0" height="30" src="http://1.bp.blogspot.com/-1QQEaqsOvzA/VYsUZL084uI/AAAAAAAABMg/KYsITID56P4/s1600/preorder_vector_binary_search.png" width="400"><br>
このジャンプのパターンは、<code>boost::container::flat_set:</code>とは違いますね。最初にアクセスされる要素は互いに近くにあり、ジャンプはだんだん長くなります。ですから、参照の局所性は<em>プロセスの最初の部分で</em>良くなっています。先ほどと正反対ですね。この新しい配置によって楽になるとは思えないでしょう。でも実際はそうなのです。<code>boost::container::flat_set:</code>の<em>ヒートマップ</em>を見てみましょう。</p><p><img border="0" height="15" src="http://4.bp.blogspot.com/-eJgJ0Uconcg/VYsWBWJ2m0I/AAAAAAAABMs/2FoX_FIIjbQ/s1600/flat_set_heat.png" width="400"><br>
この図から、平均的な二分探索の操作によって、与えられた要素がアクセスされる頻度が分かります（”より熱い”ことを示す赤色の濃い方が、ここではより頻繁にアクセスされていることを意味します）。全ての検索は中央にある要素15から始まるので、この要素を訪れる頻度は100％となり、要素7と23を訪れる頻度は50％といった具合です。一方で要素をレベル順に並べ直し、同じようなヒートマップで示すと様子が全く異なります。</p><p><img border="0" height="15" src="http://3.bp.blogspot.com/-VWfoaB0Y5FU/VYsXNoJyg2I/AAAAAAAABM4/elr4n1IyQaY/s1600/preorder_vector_heat.png" width="400"><br>
この並び順では、熱い要素がより近くに集まっているのが分かります。キャッシュ管理のメカニズムでは熱いエリアをより長くキャッシュ内に保持しようとするので、このレイアウトにすれば全体的なキャッシュミスがほとんどなくなることが期待できます。つまり、グラフの冷たい要素のエリアを利用して、熱い要素での参照の局所性を改善しているのです。</p><p>実際にこのデータ構造を検証してみましょう。プロトタイプのクラステンプレート<code>levelorder_vector&lt;T&gt;</code>を記述しました。これはレベル順にコンテンツを保存し、二分探索のためのメンバ関数を提供します。</p><p><a href="https://www.dropbox.com/s/gn5mmfqn47edlm5/levelorder_vector.cpp?dl=0" rel="nofollow" title="" class="ext-link">テストプログラム</a>があり、これを用いて<code>std::set</code>、<code>boost::container::flat_set</code>、<code>levelorder_vector</code>などの1万から300万までの要素数（<em>n</em>）となるコンテナに対する値のランダムシーケンスで<code>lower_bound</code>の実行時間を測定します。</p><p>テストを、Microsoft Visual Studio 2012のデフォルトのリリースモード設定を使ってビルドして、Intel Core i5-2520M CPU @2.50GHz環境におけるWindows boxで実行しました。値はマイクロセカンド／コンテナ内の要素数（n）で表されます。</p><p><img border="0" height="256" src="http://1.bp.blogspot.com/-bceJYXENYLw/VYukuNbvwRI/AAAAAAAABOE/xWsY03Lx6ME/s1600/binary_lookup_msvc.png" width="400"><br>
<em>注釈<br>
lower_bound execution times / number of elements：<br>
lower_boundの実行時間／要素数<br>
</em></p><p>3つのコンテナの理論上の検索時間はO(log <em>n</em>)となり、直線（水平方向の単位は対数）となるはずですが、<em>n</em>の増加に伴ってキャッシュミスが増えるためグラフが上向きに変化します。<code>levelorder_vector</code>のパフォーマンスは<code>boost::container::flat_set</code>よりも10～30％の改善度で上回っています。どちらにしても、両方とも<code>std::set</code>よりはずっと速く、<em>n</em>が5×10の5乗あたりになるまでパフォーマンスはほとんど低下しません（<code>std::set</code>は7×10の4乗あたりから低下が始まります）。この結果には上位キャッシュのフレンドリネスが直接影響しています。</p><p>Philippe Navaux、<a href="https://xckd.wordpress.com/" rel="nofollow" title="" class="ext-link">xckd</a>、<a href="https://plus.google.com/+ManuS%C3%A1nchezManu343726/posts" rel="nofollow" title="" class="ext-link">Manu Sánchez</a>の3人は、親切にも様々なLinuxプラットフォームでのGCCのテスト出力を送ってくれました。結果はどれもよく似ていたので、ここではIntel Core i7-860 @ 2.80GHz環境におけるDebianのLinux kernel 4.0でのGCC 5.1に対応する結果をお見せしましょう。</p><p><img border="0" height="256" src="http://3.bp.blogspot.com/-fN6zjHMeGIY/VYxO3KbG1eI/AAAAAAAABOY/03gQuJV59rY/s1600/binary_lookup_gcc51.png" width="400"><br>
<em>注釈<br>
lower_bound execution times / number of elements：<br>
lower_boundの実行時間／要素数<br>
</em></p><p><code>levelorder_vector</code>は<code>boost::container::flat_set</code>に比べて全体的に処理が速いのですが、説明のつかない奇妙な現象が起きています。<em>n</em>の値が小さい時には、改善度はおよそ40％ですが、その後、<em>n</em>が3×10の6乗に近づくあたりで改善度が低下し、さらにマイナスにさえなっています。この現象を説明してくれる読者がいらっしゃるとありがたいのですが。<a href="https://github.com/GrayShade" rel="nofollow" title="" class="ext-link">Laurentiu Nicola</a>は後に、<code>-march=native -O2</code>の設定で実行し、上で紹介したものとかなり異なる結果を得ました。Intel Core i7-2760QM CPU @ 2.40GHzにおけるGCC 5.1（未公開Linux ディストリビューション）のテスト結果です。</p><p><img border="0" height="256" src="http://4.bp.blogspot.com/-bslLMBZtZFw/VY3nPe0-5ZI/AAAAAAAABPg/PXo1gzt5LAA/s1600/binary_lookup_gcc51_i7_native.png" width="400"><br>
<em>注釈<br>
lower_bound execution times / number of elements：<br>
lower_boundの実行時間／要素数<br>
</em></p><p>そして以下が、Intel Celeron J1900 @ 1.99GHzにおけるGCC 5.1のテスト結果です。</p><p><img border="0" height="256" src="http://4.bp.blogspot.com/-aaRnTnjd5CI/VY3nzLtcY3I/AAAAAAAABPo/DlRqmtvJcvk/s1600/binary_lookup_gcc51_celeron_native.png" width="400"><br>
<em>注釈<br>
lower_bound execution times / number of elements：<br>
lower_boundの実行時間／要素数<br>
</em></p><p><code>-march=native</code>によって大きな違いが出るようです。<em>n</em>が3×10の6乗に近づくあたりで、<code>levelorder_vector</code>の処理時間は<code>boost::container::flat_set</code>に比べて最大25％（i7）または、40％（Celeron）速くなります。しかし、<em>n</em>の値が小さい時には、大差はありません。</p><p><a href="https://www.blogger.com/profile/16286395755438527078" rel="nofollow" title="" class="ext-link">fortch</a>が提供してくれたテスト結果によると、Intel Core i7-3740QM CPU @ 2.70GHz 環境でのDarwin 13.1.0 boxにおける<code>-std=c++0x</code>モードのclang-500.2.79では、以下のようになります。</p><p><img border="0" height="256" src="http://2.bp.blogspot.com/-5X5A65Akvgk/VYxS-4x7bJI/AAAAAAAABOk/29dXzjxRZ9Q/s1600/binary_lookup_clang500279_darwin.png" width="400"><br>
<em>注釈<br>
lower_bound execution times / number of elements：<br>
lower_boundの実行時間／要素数<br>
</em></p><p><code>boost::container::flat_set</code>に対する<code>levelorder_vector</code>のパフォーマンスの改善度は、10％から20％です。</p><p>Manu Sánchezは、Intel Core i7 4790k @4.5GHz環境のArch Linux 3.16 boxにおけるClang 3.6.1でテストを行いました。最初は、<a href="https://gcc.gnu.org/libstdc++/" rel="nofollow" title="" class="ext-link">libstdc++-v3</a>標準ライブラリを使用しました。</p><p><img border="0" height="256" src="http://4.bp.blogspot.com/-JkZOfjxZIzA/VYxVTILGU5I/AAAAAAAABOw/CeHqB2_g7_Y/s1600/binary_lookup_clang361_libstdc%252B%252B_arch.png" width="400"><br>
<em>注釈<br>
lower_bound execution times / number of elements：<br>
lower_boundの実行時間／要素数<br>
</em></p><p>次に、<a href="http://libcxx.llvm.org/" rel="nofollow" title="" class="ext-link">libc++</a>で再びテストを行いました。</p><p><img border="0" height="256" src="http://1.bp.blogspot.com/-M_uKlIY8i9A/VYxVn0eHjvI/AAAAAAAABO4/c1aWd-ljIvk/s1600/binary_lookup_clang361_libc%252B%252B_arch.png" width="400"><br>
<em>注釈<br>
lower_bound execution times / number of elements：<br>
lower_boundの実行時間／要素数<br>
</em></p><p>似たようなテスト結果が出ました。<code>levelorder_vector</code>の処理速度は<code>boost::container::flat_set</code>よりも5％から20％速くなっています。これら2つのコンテナで、libc++を使用した時にはグラフ上にはっきりと凹凸が見られます。この凹凸は、OS XにおけるClangでテストした際にも現れていますが、標準ライブラリにlibstdc++-v3を使用した場合は現れていません。ということは、この凸凹は、libc++のメモリアロケータに関連する何かが作用しているせいかもしれません。<br>
Laurentiuが行った<code>-march=native -O2</code>のClang 3.6.1 でのテスト結果は、先ほどお見せした、Manuが行ったIntel Celeron J1900 @ 1.99GHz環境におけるi7CPUでの結果と似ています。</p><p><img border="0" height="256" src="http://4.bp.blogspot.com/-oUHuVq3Wp-A/VY3qdfS8DUI/AAAAAAAABP8/nKBvdJtq9Ts/s1600/binary_lookup_clang361_celeron_native.png" width="400"><br>
<em>注釈<br>
lower_bound execution times / number of elements：<br>
lower_boundの実行時間／要素数<br>
</em></p><p><code>levelorder_vector</code>の処理速度は<code>boost::container::flat_set</code>よりも最大35％速くなっています。</p><p>ソートされた配列に対する二分探索の方がノードベースのデータ構造に対する処理速度より速いとしても、代替手段の連続レイアウトの方が、さらにキャッシュフレンドリーです。ソートされた数列を持つ二分木の幅優先で、要素の配列に基づく様々な可能性を見てきました。このデータ構造は、パフォーマンスは低いかもしれませんが、追加や削除などの操作ではどうでしょうか。このテーマについては、今後の記事で考察したいと思います。</p>
