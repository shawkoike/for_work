<p><a href="https://www.ibm.com/developerworks/community/blogs/jfp/entry/Python_Meets_Julia_Micro_Performance?lang=ja" rel="nofollow" title="" class="ext-link">How To Make Python Run As Fast As Julia</a> （2015-12-01） by <a href="https://www.ibm.com/developerworks/community/profiles/html/profileView.do?userid=2700028FGP&amp;lang=ja" rel="nofollow" title="" class="ext-link">Jean Francois Puget</a><br>
Originally published on <a href="http://www.ibm.com/developerworks/jp/" rel="nofollow" title="" class="ext-link">IBM developerWorks</a></p><p>科学技術計算には、Pythonなどの言語よりもJuliaを使った方がいいのでしょうか？　<a href="http://julialang.org/" rel="nofollow" title="" class="ext-link">http://julialang.org/</a>に載っているベンチマークを見ると、どうしてもそんな風に思ってしまいます。というのも、Pythonなどの高水準言語は、スピード面で大幅に劣っているのです。けれども、これは私が最初に感じた疑問ではありません。私が気になったのは、「Juliaのチームが書いたPythonのベンチマークは、Pythonに最適なものだったのか？」ということです。</p><p>こういった多言語の比較について、私の考えを述べましょう。まずベンチマークというのは、実行するタスクによって定義されるものです。よって、そのタスクを実行するための最適なコードを、各言語に精通した人々が最善を尽くして書かなくてはなりません。仮に、特定の言語を専門とするチームが全てのコードを書いてしまうと、それ以外の言語が最適に使用されない恐れがあるからです。</p><p>Juliaのチームが、<a href="https://github.com/JuliaLang/julia/tree/master/test/perf/micro" rel="nofollow" title="" class="ext-link">使用したコード</a>をGithub上に公開したのは賢明だと思います。なお、Pythonのコードは<a href="https://github.com/JuliaLang/julia/blob/master/test/perf/micro/perf.py" rel="nofollow" title="" class="ext-link">こちら</a>で確認できます。</p><p>このコードを一目見て分かったのは、私が懸念していた偏りがあるということです。コードはC言語のスタイルで書かれていて、配列やリストに対してループ処理がたくさん使われていたのです。これはPythonの最適な使用法ではありません。</p><p>この点においては、私もずっと同じ過ちを犯してきたので、Juliaのチームを批判する気はありません。ただ、「<a href="https://www.ibm.com/developerworks/community/blogs/jfp/entry/Python_Is_Not_C?lang=en" rel="nofollow" title="" class="ext-link">PythonはCではない</a>」という記事にも書いたように、私は痛い教訓を学びました。Pythonでは配列やリストをループ処理することは何としてでも避けるべきで、さもないと実行速度が極端に落ちてしまうという教訓です。</p><p>では、上記のようにコードがC言語のスタイルに偏っているのなら、PythonやPythonツールをもっと有効に使えば、ベンチマークを改善できるのでしょうか？　これは、（少なくとも私にとっては）興味深い疑問です。</p><p>その答えは下に書いてあるのですが、ここでひと言、私は決してJuliaを見下そうとしているわけはないと言わせてください。Juliaは今後も更に進化を遂げ、改良されるでしょうし、間違いなく注目する価値のある言語です。ただ私は、Python寄りの視点で物事を見ようとしているだけなのです。まあ、実際のところは、こんな風に理由をつけて、コードの実行速度を上げるために、いろいろなPythonツールを模索しているわけです。</p><p>なお、以下に記載している内容は、Python 3.5.1と<a href="https://www.google.fr/url?sa=t&amp;rct=j&amp;q=&amp;esrc=s&amp;source=web&amp;cd=1&amp;cad=rja&amp;uact=8&amp;ved=0ahUKEwi1n9KroLfJAhXFuhQKHbMhA-8QFggeMAA&amp;url=https%3A%2F%2Fwww.continuum.io%2Fdownloads&amp;usg=AFQjCNH5KKA7CTASoQKpNBeQAV2xSKKTrQ" rel="nofollow" title="" class="ext-link">Anaconda</a>を使って、Windowsマシン上で実行しました。以下の全てのベンチマークに用いた全コードを含むノートブックは<a href="https://gist.github.com/jfpuget/b53f1e15a37aba5944ad" rel="nofollow" title="" class="ext-link">github</a>と<a href="http://nbviewer.jupyter.org/gist/jfpuget/b53f1e15a37aba5944ad" rel="nofollow" title="" class="ext-link">nbviewer</a>にあります。</p><p>ソーシャルメディア上でさまざまなコメントが寄せられているので補足しますが、私はここではC言語のコードを一切書いていません。信じられないという方は、セミコロンを探してみてください。このブログで使っているツールは全て、Anacondaや他のディストリビューションで入手できる標準的なCPythonの実装で動かしています。また、以下に載せたコードはどれも、<a href="https://gist.github.com/jfpuget/b53f1e15a37aba5944ad" rel="nofollow" title="" class="ext-link">単一のノートブック</a>で実行しています。 <a href="https://github.com/JuliaLang/julia/blob/master/test/perf/micro/perf.jl" rel="nofollow" title="" class="ext-link">Github</a>からJuliaのマイクロパフォーマンスファイルを使おうとしたのですが、そのままではJulia 0.4.2では動かなかったので、ファイルを編集して@timeitを@timeに置換しています。更に測定時間にコンパイル時間が含まれないよう、測定前に計測対象の関数を呼び出す処理も追加しました。これをJuliaのコマンドラインインターフェースを使って、Pythonを動作させているのと同じマシンで実行しました。</p><p>Juliaのチームが使った1つ目のベンチマークは、単純なフィボナッチ関数のコードです。</p><p>この関数の値はnが大きくなるにつれて、以下のように急激に増加します。</p><p>ここで注目していただきたいのは、Pythonの任意精度が役に立っているという点です。同じものをCのような言語で書くとなると、整数オーバーフローを防ぐため、コーディングで苦労するはずです。またJuliaであれば、BigInt型を使用する必要があるでしょう。</p><p>Juliaのベンチマークは、どれも実行時間を測るものです。以下はJuliaでBigIntを使用した場合と、使用していない場合のそれぞれの実行時間です。</p><p>Pythonのノートブックで実行時間を取得する方法の1つに、<code>%timeit</code>というマジックコマンドを使う方法があります。例えば、次のように新しいセルに入力します。</p><p>すると以下の実行結果が得られます。</p><p>この場合、タイマーは以下の処理を行っています。</p><p>ループのサイズ（ここでは100ループを3回）は、タイマーによって自動調整される値で、測定するコードの実行速度に応じて変化します。</p><p>bigintを使用した場合のJuliaに比べて、Pythonの実行時間はかなり短く、値はそれぞれ12ミリ秒と3ミリ秒です。つまり任意精度を使用した場合、Pythonの処理速度はJuliaの4倍ということになります。</p><p>それでも、Juliaのデフォルトの64ビット整数に比べると、まだまだPythonは遅いと言えます。では、Pythonで強制的に64ビット整数を使う方法を見ていきましょう。</p><p>64ビット整数を使う方法の1つとして、<a href="http://docs.cython.org/" rel="nofollow" title="" class="ext-link">Cython</a>コンパイラの使用が挙げられます。このコンパイラはPythonで書かれており、以下のようにするとインストールできます。</p><p>Anacondaを使っている場合は、インストール方法が異なります。少しややこしいので、詳細は「<a href="https://www.ibm.com/developerworks/community/blogs/jfp/entry/Installing_Cython_On_Anaconda_On_Windows?lang=en" rel="nofollow" title="" class="ext-link">Windows上にAnaconda用のCythonをインストールする</a>」というブログ記事に書きました。<br>
インストールが完了したら、<code>%load_ext</code>マジックを使ってCythonをノートブックに読み込みます。</p><p>これで、ノートブック上でコードをコンパイルできます。あとはコンパイルしたいコード一式を1つのセルに入れ、必要なインポート宣言を含めて、セルマジック<code>%%cython</code>で、そのセルを開始するだけです。</p><p>このセルを実行すると、シームレスにコードがコンパイルされます。ここでは、Cythonでコンパイルされたことを示すために、関数に少し違う名前を付けています。もちろん一般的には、こうする必要はありません。元の関数を、同じ名前のコンパイル後の関数で置き換えても大丈夫です。</p><p>実行時間は以下の通りです。</p><p>なんと元のPythonのコードと比べて、2倍以上も速くなりました！　BigInt型を使ったJuliaと比べると、9倍の速さです。</p><p>では次に、静的型付けをして試してみましょう。関数の宣言には、<code>def</code>の代わりに<code>cpdef</code>というキーワードを使います。こうすると、該当するC言語の型で、関数のパラメータを型付けすることが可能です。コードは以下のようになります。</p><p>セルを実行すると、以下の実行時間が得られます。</p><p>驚くべき結果です。24マイクロ秒ということは、元のベンチマークと比べて、約150倍も速くなっています！　これは、Juliaを使った際の80マイクロ秒に引けを取らない値ですね。</p><p>皆さんの中には、静的型付けはPythonの目的にそぐわないと意義を唱える方もいるでしょう。これには私も大筋で賛成しているので、後ほどパフォーマンスを犠牲にせずに、この問題を回避する方法について説明します。でも、ここでの問題は別のことでしょう。本来、フィボナッチ関数は整数で呼び出されるべきものです。そして、静的型付けによって損なわれるのは、Pythonの持つ任意精度です。フィボナッチの場合は、パラメータが大きすぎると整数オーバーフローが起きる恐れがあるので、C言語の<code>long</code>型を使って入力パラメータのサイズに制限をかけます。<br>
ここで重要なのは、Juliaの計算には64ビット整数も使われているので、それぞれ静的型付けをしたバージョンのPythonとJuliaを比較するのが公平だということです。</p><p>Pythonの任意精度が保たれていれば、更に良い結果が出ます。<code>fib</code>関数は、同じ計算を何度も繰り返す関数です。例えば、<code>fib(20)</code>は<code>fib(19)</code>と<code>fib(18)</code>を呼び出し、その次に<code>fib(19)</code>は<code>fib(18)</code>と<code>fib(17)</code>を呼び出します。その結果<code>fib(18)</code>は2回呼び出されます。少し考えると分かりますが、<code>fib(17)</code>は3回、<code>fib(16)</code>は5回…と呼び出されます。</p><p>Python 3では、functoolsの標準ライブラリを使うと、このような重複した計算をせずに済みます。</p><p>この関数の実行時間は以下の通りです。</p><p>更に速度が190倍になって、元のPythonのコードと比べると約30,000倍も速くなりました！　単に再帰関数に注釈を加えただけであることを考えると、これは素晴らしい結果です。</p><p>なお、Python 2.7では、このように自動でキャッシュはできません。よって、重複して計算しないようにするには、明示的にコードを書き換える必要があります。</p><p>このコードでは、2つのローカル変数を同時に割り当てるPythonの特性を生かしている点に注目してください。実行時間は次の通りです。</p><p>こちらでは速度が20倍になりました！　では静的型付けをした場合と、していない場合について、それぞれ関数をコンパイルしてみましょう。ここでは、ローカル変数を型付けするために、<code>cdef</code>キーワードをどのように使っているかに着目してください。</p><p>以下のようにすると、1つのセルにある2つのバージョンの実行時間を測定できます。</p><p>結果は以下のようになります。</p><p>静的型付けをしたコードは82ナノ秒で、元のベンチマークと比べると約45,000倍も速くなりました！</p><p>任意の入力に対してフィボナッチ数を計算したい場合は、型付けをしないバージョンを使い続けるのが良いでしょう。それでも実行速度は30,000倍になるので、まずまずだと言えますよね。</p><p>次は、<a href="http://numba.pydata.org/" rel="nofollow" title="" class="ext-link">Numba</a>という別のツールを使ってみましょう。これはPythonのサブセット用の実行時（JIT）コンパイラです。今はまだ全てのPython上で動作するわけではありませんが、きちんと動けば素晴らしいことができるツールです。</p><p>Numbaのインストールは手間がかかるので、<a href="https://www.google.fr/url?sa=t&amp;rct=j&amp;q=&amp;esrc=s&amp;source=web&amp;cd=1&amp;cad=rja&amp;uact=8&amp;ved=0ahUKEwi1n9KroLfJAhXFuhQKHbMhA-8QFggeMAA&amp;url=https%3A%2F%2Fwww.continuum.io%2Fdownloads&amp;usg=AFQjCNH5KKA7CTASoQKpNBeQAV2xSKKTrQ" rel="nofollow" title="" class="ext-link">Anaconda</a>やなどのPythonディストリビューションや、既にNumbaがインストールされている<a href="https://www.ibm.com/developerworks/community/blogs/jfp/entry/Using_Docker_Machine_On_Windows?lang=en" rel="nofollow" title="" class="ext-link">Dockerイメージ</a>を使うことをお勧めします。インストールが完了したら、NumbaのJITコンパイラをインポートします。</p><p>使い方は非常にシンプルで、コンパイルしたい関数をデコレートするだけです。コードは次のようになります。</p><p>実行時間は以下の通りです。</p><p>これは型付けをしないCythonのコードよりも速く、元のPythonのコードと比べると、約17000倍の速度です！</p><p>では、ここで2つ目のベンチマークを見てみましょう。以下は、Juliaチームが使用したPythonのコードで、クイックソートのアルゴリズムが実装されています。</p><p>彼らのベンチマークのコードを、関数でラップしてみます。</p><p>実行時間は以下の通りです。</p><p>上記のコードはC言語のコードにそっくりですね。これならCythonがうまく動くはずです。ではCythonと静的型付けを使い、リストの代わりにNumPy配列も使ってみましょう。実際のところサイズが大きく、要素数が何千とある場合は、Pythonのリストよりも<a href="http://www.numpy.org/" rel="nofollow" title="" class="ext-link">NumPy</a>配列の方が速いのです。</p><p>NumPyのインストールには時間がかかるので、既にPythonの科学計算用のスタックがインストールされた<a href="https://www.google.fr/url?sa=t&amp;rct=j&amp;q=&amp;esrc=s&amp;source=web&amp;cd=1&amp;cad=rja&amp;uact=8&amp;ved=0ahUKEwi1n9KroLfJAhXFuhQKHbMhA-8QFggeMAA&amp;url=https%3A%2F%2Fwww.continuum.io%2Fdownloads&amp;usg=AFQjCNH5KKA7CTASoQKpNBeQAV2xSKKTrQ" rel="nofollow" title="" class="ext-link">Anaconda</a>や<a href="https://www.ibm.com/developerworks/community/blogs/jfp/entry/Using_Docker_Machine_On_Windows?lang=en" rel="nofollow" title="" class="ext-link">Dockerイメージ</a>などを使うことをお勧めします。</p><p>Cythonを使う際は、Cythonを適用するセル内で、NumPyをインポートしなければなりません。NumPy配列は、配列要素の型を示す特別なシンタックスと、配列の次元数（1次元や2次元など）によって宣言されます。デコレータを用いてCythonの境界チェックを取り除きます。</p><p><coode>benchmark_qsort_numpy_cython()関数の実行時間は以下の通りです。</coode></p><p>元のベンチマークに比べると、速度は約23倍になりました。しかし、まだPythonを最良の方法で使っているとは言えません。Pythonの力を最大限引き出すには、NumPyのビルトイン関数である<code>sort()</code>を使います。この関数は、デフォルトでクイックソートのアルゴリズムを使用します。では、次のコードの実行時間を測定してみましょう。</p><p>結果は以下の通りです。</p><p>元のベンチマークに比べると、速度は58倍にもなりました！　このベンチマークにおけるJuliaの結果は419マイクロ秒なので、コンパイルしたPythonの速度はJulia の1.4倍ということになります。</p><p>皆さんの中には、同一条件で比較していないじゃないかと言う方もいるでしょうが、そんなことはありません。ここでの課題は、ホスト言語を使って、最良の方法で入力配列をソートすることでしたよね。この場合、最良の方法とは、ビルトインの関数を使うことなのです。</p><p>では次に、マンデルブロ集合計算を行う3番目の例を見てみましょう。以下はJuliaチームが使ったPythonのコードです。</p><p>最後の行はサニティーチェックです。<code>mandelperf()</code>関数の実行時間は以下のようになります。</p><p>以下は、Cythonを使った場合の結果です。</p><p>悪くはありませんが、Numbaを使えばもっと良い値が出ます。しかし、残念なことにNumbaはリストの内包表記をコンパイルできません。そのため2番目の関数にはNumbaを使えないので、1番目の関数にだけ適用します。すると、コードは以下のようになります。</p><p>実行時間は以下の通りです。</p><p>速度はCythonの4倍、元のPythonのコードの9倍なので、なかなかの結果です。</p><p>では更に速度を上げることはできるのでしょうか？　それを知る方法の1つが、コードのプロファイリングです。この場合、ビルトインの<code>%prun</code>プロファイラでは精度が低いので、より高機能な<a href="https://github.com/rkern/line_profiler" rel="nofollow" title="" class="ext-link">line_profiler</a>というプロファイラを使う必要があります。これは以下のように、pipでインストールできます。</p><p>インストールが完了したら、line_profilerを読み込みます。</p><p>これで、以下のマジックコマンドを使うと関数の解析が行えます。</p><p>すると以下の結果がポップアップ画面に表示されます。</p><p>これを見ると、<code>mandelperf_numba()</code>関数の最初の行と最後の行で、大半の時間が費やされていることが分かります。最後の行は少し複雑なので、2つに分割して再度解析してみましょう。</p><p>以下はプロファイラの出力結果です。</p><p>この結果から、<code>mandel_numba()</code>関数の呼び出しに要する時間は、総実行時間の4分の1に過ぎないことが分かります。残りの時間は<code>mandelperf_numba()</code>関数で費やされているので、この関数を最適化してみる価値はあるでしょう。</p><p>ここではCythonはあまり役に立ちませんし、Numbaは使えません。このジレンマから脱するために、もう一度NumPyを使いましょう。以下の内容をNumPyのコードに書き換えて、同じ結果が出力されるようにします。</p><p>これは、いわゆる2Dメッシュを作成するコードで、<code>r1</code>と<code>r2</code>で座標指定される点の複素数表示を算出します。点Pijの座標は<code>r1[i]</code>と<code>r2[j]</code>です。Pijは複素数<code>r1[i] + 1j*r2[j]</code>で表され、特殊な定数<code>1j</code>は虚数単位iを表します。</p><p>この計算は、そのまま以下のようにコーディングできます。</p><p>ここでは、戻り値を整数の二次元配列に変換している点に注目してください。このようにした方が、結果を表示する可能性がある場合は便利です。</p><p>実行時間は以下の通りです。</p><p>元のPythonのコードに比べると、速度は約50倍になっています！　このベンチマークにおけるJuliaの結果は196マイクロ秒なので、Pythonの速度はJuliaの1.6倍ということになります。</p><p>[2016.2.2 追記]これよりも更にPythonの速度を上げる方法については、「<a href="https://www.ibm.com/developerworks/community/blogs/jfp/entry/How_To_Compute_Mandelbrodt_Set_Quickly?lang=en" rel="nofollow" title="" class="ext-link">Pythonを使って高速でマンデルブロ集合計算を行う方法</a>」という記事をご覧ください。</p><p>もう1つ別の例を見てみましょう。正直なところ、以下のコードで何を計測したのか私にはよく分かりませんが、こちらがJuliaチームが使用したコードです。</p><p>実際のJuliaチームのコードには、最後に「L」があった場合は、それを取るという命令があります。その行はAnacondaのインストール環境には必要ですが、Python 3のインストール環境には不要なので、私は削除しました。以下が元のコードです。</p><p>修正したコードの実行時間は以下の通りです。</p><p>NumbaとCythonは役に立たないようでした。<br>
このベンチマークの結果を見て途方に暮れた私は、元のコードをプロファイリングすることにしました。以下が、その結果です。</p><p>この結果から、乱数の生成処理が大半の時間を占めていることが分かります。果たしてこれが、このベンチマークの意図なのでしょうか？</p><p>速度を上げるために、NumPyを使って、乱数の生成処理をループの外に出し、1ステップで乱数の配列を生成するようにします。NumPyがCのintを使うので、最大の値を2^31 – 1に制限する必要があります。</p><p>実行時間は以下の通りです。</p><p>悪くありませんね。4倍近い速さで、Cythonコードぐらいの速度です。</p><p>いったん配列を取得してから、それをループ処理し、要素1ずつに<code>hex()</code>と<code>int()</code>関数を適用するのは一見無駄なことのようにも見えます。ありがたいことに、Numpyはループというより、むしろ配列に対し関数を呼び出す方法を提供します。すなわち<code>numpy.vectorize()</code>関数です。この関数は、一度に1つのオブジェクトを操作する関数を入力としてとります。これは配列に作用する新しい関数を返します。</p><p>このコードは、更に速く実行でき、Cythonコードとほぼ同じ速さです。</p><p>Cythonでさらにこれを高速化できます。</p><p>計測結果は以下の通りです。</p><p><!-- Pythonに長けた人なら、Pythonの構文解析にそれほど明るくない私よりも、もっとうまくできるでしょう。ですが、ここではPythonのループを避けることは賢明な考えであるということが再度証明されました。
 --></p><p>Juliaチームが用いた4つの例について速度を上げる方法を解説してきました。それに加えて更に3つの方法があります。</p><p>以上の7つの例の全てのコードを含んだノートブックは<a href="https://gist.github.com/jfpuget/b53f1e15a37aba5944ad" rel="nofollow" title="" class="ext-link">github</a>と<a href="http://nbviewer.jupyter.org/gist/jfpuget/b53f1e15a37aba5944ad" rel="nofollow" title="" class="ext-link">nbviewer</a>で公開しています。</p><p>表でまとめてみましょう。元のPythonコードと最適化されたコードの間で得た速度アップを示しています。また同時にJuliaチームが用いたそれぞれのベンチマーク例に対し、私たちが使ったツールを示しています。</p><p>この表が示すのは、最適化されたPythonコードは最初の6つの例ではJuliaより速いということと、残りの2つの例ではJuliaより遅いということです。留意すべきは、私は公平を期するために、フィナボッチには再帰的コードを使っていることです。</p><p>このようなマイクロ･ベンチマークが、どの言語が最も速いかという明確な答えを与えてくれるとは思いません。例えばrandmatstatの例は5×5の行列を処理します。そのためにNumpy配列を使うことは、過剰です。より大きな行列をベンチマークでテストしなければなりません。</p><p>私はより複雑なコード上で、言語のベンチマークテストをしなければならないと思っています。それに関するいい例が、「<a href="http://tullo.ch/articles/python-vs-julia/?cm_mc_uid=93691459461014413924647&amp;cm_mc_sid_50200000=1454271221" rel="nofollow" title="" class="ext-link">Julia対Python 機械学習の一例</a>」という記事に載っています。この記事では、JuliaはCythonを上回るようです。時間があればNumbaで試してみたいと思います。</p><p>とにかく、マイクロ・ベンチマーク上で、正しいツールが使われる時、PythonのパフォーマンスはJuliaのパフォーマンスに匹敵すると言ってもいいでしょう。反対に、JuliaのパフォーマンスはコンパイルされたPythonのパフォーマンスに匹敵すると言うこともできます。コードに注釈と修正を加える必要もなく、Juliaがこのように動作するとすれば、それ自体は興味深いです。</p><p>少し休憩しましょう。ここまでPythonコードのパフォーマンスが重要な時に使用すべき数多くのツールについて見てきました。</p><p>これらのツールを使ってみて、どのように役に立つのかを実感してください。それと同時に賢くツールを使ってください。どこで最適化するのが価値があるかに集中するために、コードをプロファイルしてください。実際、速度を上げるためにコードを書き直すことは、時にコードを難読化したり、融通を利きにくくしたりさせます。そのため、結果として速度アップが意味のある時だけ、行うようにしてください。Donald Knuthはこの点を、次のように、うまくアドバイスしています。</p><p>ですが、Knuthの引用は「最適化は価値がない」と言っているわけではないことに注意して、「<a href="http://www.joshbarczak.com/blog/?p=580&amp;cm_mc_uid=93691459461014413924647&amp;cm_mc_sid_50200000=1454275774" rel="nofollow" title="" class="ext-link">Knuthの誤った引用はやめよう</a>」や「<a href="http://joeduffyblog.com/2010/09/06/the-premature-optimization-is-evil-myth/?cm_mc_uid=93691459461014413924647&amp;cm_mc_sid_50200000=1454275774" rel="nofollow" title="" class="ext-link">早まった最適化は諸悪の根源という神話</a>」といった記事を読んでみてください。</p><p>Pythonコードは、最適化が意味のある時と場所において最適化できるものであり、されるべきものです。<br>
最後に私が使ったものや、それ以外のツールについて述べている面白い記事を載せておきます。</p>
